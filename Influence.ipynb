{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "import numpy as np\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()},\n",
    "                  reload_support=True)\n",
    "from algorithms.knn_neighborhood import UserKNN\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime as dt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    top_n_recommendations = defaultdict(list)\n",
    "    for ruid, riid, true_r, est_r, _ in predictions:\n",
    "        top_n_recommendations[ruid].append((riid, est_r))\n",
    "\n",
    "    for ruid, user_ratings in top_n_recommendations.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n_recommendations[ruid] = [riid for riid, _ in user_ratings[:n]]\n",
    "\n",
    "    return top_n_recommendations\n",
    "\n",
    "def get_mae(predictions):\n",
    "    errors = defaultdict(list)\n",
    "    for ruid, _, true_r, est_r, _ in predictions:\n",
    "        errors[ruid].append(np.abs(true_r - est_r))\n",
    "    return {ruid: np.mean(errors[ruid]) for ruid in errors.keys()}\n",
    "\n",
    "def measure_influence_top_n(base_top_n, top_n):  \n",
    "    jdists = []\n",
    "    for ruid in top_n.keys():\n",
    "        jsim = len(set(top_n[ruid]).intersection(base_top_n[ruid])) / len(set(top_n[ruid]).union(base_top_n[ruid]))\n",
    "        jdists.append(1 - jsim)\n",
    "    return np.mean(jdists)\n",
    "\n",
    "def measure_influence_mae(base_mae, mae):\n",
    "    dists = [mae[ruid] - base_mae[ruid] for ruid in mae.keys()]\n",
    "    return np.mean(dists)\n",
    "\n",
    "def get_top_n_mentors(model, n=10):\n",
    "    nr_of_students = [(model.trainset.to_raw_uid(iuid), len(students)) for iuid, students in model.students.items()]\n",
    "    top_n_mentors = sorted(nr_of_students, key=lambda t: t[1])[::-1][:n]\n",
    "    top_n_mentors = [ruid for ruid, _ in top_n_mentors]\n",
    "    \n",
    "    return top_n_mentors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"], usecols=[\"user_id\", \"item_id\", \"rating\"])\n",
    "data_df[\"user_id\"] = data_df[\"user_id\"].map({b: a for a, b in enumerate(data_df[\"user_id\"].unique())})\n",
    "data_df[\"item_id\"] = data_df[\"item_id\"].map({b: a for a, b in enumerate(data_df[\"item_id\"].unique())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train- and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2)\n",
    "dataset = Dataset.load_from_df(data_df, reader=reader)\n",
    "raw_trainset = [(ruid, riid, r, None) for ruid, riid, r in train_df.to_records(index=False)]\n",
    "raw_testset = [(ruid, riid, r, None) for ruid, riid, r in test_df.to_records(index=False)]\n",
    "trainset = Dataset.construct_trainset(dataset, raw_trainset)\n",
    "testset = Dataset.construct_testset(dataset, raw_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = UserKNN().compute_similarities(trainset, min_support=1)\n",
    "pop = UserKNN().compute_popularities(trainset)\n",
    "gain = UserKNN().compute_gain(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_top_n = defaultdict(list)\n",
    "base_mae = defaultdict(list)\n",
    "top_n_mentors = defaultdict(list)\n",
    "for k in Ks:\n",
    "    # UserKNN\n",
    "    model = UserKNN(k=k, precomputed_sim=sim)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"UserKNN\"].append(get_top_n(predictions))\n",
    "    base_mae[\"UserKNN\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"UserKNN\"].append(get_top_n_mentors(model))\n",
    "    \n",
    "    # UserKNN + reuse\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"UserKNN + Reuse\"].append(get_top_n(predictions))\n",
    "    base_mae[\"UserKNN + Reuse\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"UserKNN + Reuse\"].append(get_top_n_mentors(model))\n",
    "    \n",
    "    # Popularity\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"Popularity\"].append(get_top_n(predictions))\n",
    "    base_mae[\"Popularity\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"Popularity\"].append(get_top_n_mentors(model))\n",
    "    \n",
    "    # Popularity + Reuse\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"Popularity + Reuse\"].append(get_top_n(predictions))\n",
    "    base_mae[\"Popularity + Reuse\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"Popularity + Reuse\"].append(get_top_n_mentors(model))\n",
    "    \n",
    "    # Gain\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, precomputed_gain=gain, tau_4=0.5)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"Gain\"].append(get_top_n(predictions))\n",
    "    base_mae[\"Gain\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"Gain\"].append(get_top_n_mentors(model))\n",
    "    \n",
    "    # Gain + reuse\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, precomputed_gain=gain, tau_4=0.5, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    base_top_n[\"Gain + Reuse\"].append(get_top_n(predictions))\n",
    "    base_mae[\"Gain + Reuse\"].append(get_mae(predictions))\n",
    "    top_n_mentors[\"Gain + Reuse\"].append(get_top_n_mentors(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mentors = set()\n",
    "for mentors in top_n_mentors.values():\n",
    "    all_mentors = all_mentors.union(set(np.ravel(mentors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User id: 8, Time elapsed: 0:02:25.560089\n",
      "User id: 777, Time elapsed: 0:04:51.775546\n",
      "User id: 10, Time elapsed: 0:05:26.596507\n",
      "User id: 267, Time elapsed: 0:05:52.115092\n",
      "User id: 525, Time elapsed: 0:08:39.629677\n",
      "User id: 766, Time elapsed: 0:11:16.504791\n",
      "User id: 13, Time elapsed: 0:11:40.083069\n",
      "User id: 17, Time elapsed: 0:13:51.766741\n",
      "User id: 19, Time elapsed: 0:16:07.075688\n",
      "User id: 276, Time elapsed: 0:18:16.962943\n",
      "User id: 532, Time elapsed: 0:19:24.547771\n",
      "User id: 795, Time elapsed: 0:21:38.675902\n",
      "User id: 31, Time elapsed: 0:24:08.923954\n",
      "User id: 544, Time elapsed: 0:26:39.861004\n",
      "User id: 33, Time elapsed: 0:29:13.617194\n",
      "User id: 288, Time elapsed: 0:31:49.754727\n",
      "User id: 37, Time elapsed: 0:34:32.867067\n",
      "User id: 39, Time elapsed: 0:37:13.686184\n",
      "User id: 808, Time elapsed: 0:40:01.248195\n",
      "User id: 554, Time elapsed: 0:42:43.429205\n",
      "User id: 555, Time elapsed: 0:45:15.818427\n",
      "User id: 301, Time elapsed: 0:47:30.684874\n",
      "User id: 815, Time elapsed: 0:49:48.869245\n",
      "User id: 303, Time elapsed: 0:50:49.112992\n",
      "User id: 565, Time elapsed: 0:53:30.304885\n",
      "User id: 566, Time elapsed: 0:55:54.145735\n",
      "User id: 311, Time elapsed: 0:58:05.848193\n",
      "User id: 312, Time elapsed: 1:00:49.810129\n",
      "User id: 828, Time elapsed: 1:03:26.431777\n",
      "User id: 573, Time elapsed: 1:05:49.640935\n",
      "User id: 60, Time elapsed: 1:06:27.593217\n",
      "User id: 838, Time elapsed: 1:09:05.780467\n",
      "User id: 840, Time elapsed: 1:11:18.869398\n",
      "User id: 73, Time elapsed: 1:13:51.324569\n",
      "User id: 841, Time elapsed: 1:16:17.250062\n",
      "User id: 75, Time elapsed: 1:18:27.514781\n",
      "User id: 330, Time elapsed: 1:20:50.639183\n",
      "User id: 331, Time elapsed: 1:23:28.804639\n",
      "User id: 332, Time elapsed: 1:26:01.792702\n",
      "User id: 79, Time elapsed: 1:28:23.555478\n",
      "User id: 334, Time elapsed: 1:29:22.194231\n",
      "User id: 592, Time elapsed: 1:29:56.209929\n",
      "User id: 82, Time elapsed: 1:32:12.372656\n",
      "User id: 338, Time elapsed: 1:34:41.036561\n",
      "User id: 851, Time elapsed: 1:37:02.315208\n",
      "User id: 850, Time elapsed: 1:39:27.105971\n",
      "User id: 849, Time elapsed: 1:39:48.509035\n",
      "User id: 90, Time elapsed: 1:40:32.631308\n",
      "User id: 605, Time elapsed: 1:42:54.794809\n",
      "User id: 862, Time elapsed: 1:45:19.109892\n",
      "User id: 94, Time elapsed: 1:45:36.944657\n",
      "User id: 864, Time elapsed: 1:46:07.954008\n",
      "User id: 99, Time elapsed: 1:48:09.502746\n",
      "User id: 611, Time elapsed: 1:50:14.904918\n",
      "User id: 869, Time elapsed: 1:52:18.593735\n",
      "User id: 613, Time elapsed: 1:52:37.295272\n",
      "User id: 105, Time elapsed: 1:54:35.861278\n",
      "User id: 620, Time elapsed: 1:56:45.136930\n",
      "User id: 111, Time elapsed: 1:59:07.971140\n",
      "User id: 880, Time elapsed: 1:59:33.171266\n",
      "User id: 116, Time elapsed: 2:01:36.158776\n",
      "User id: 885, Time elapsed: 2:03:36.449829\n",
      "User id: 374, Time elapsed: 2:05:22.728826\n",
      "User id: 631, Time elapsed: 2:07:09.149647\n",
      "User id: 375, Time elapsed: 2:07:26.583306\n",
      "User id: 122, Time elapsed: 2:09:27.799629\n",
      "User id: 378, Time elapsed: 2:10:52.783092\n",
      "User id: 636, Time elapsed: 2:12:49.636501\n",
      "User id: 893, Time elapsed: 2:14:50.333710\n",
      "User id: 381, Time elapsed: 2:16:32.863566\n",
      "User id: 379, Time elapsed: 2:16:50.813291\n",
      "User id: 896, Time elapsed: 2:18:33.822763\n",
      "User id: 640, Time elapsed: 2:19:06.544074\n",
      "User id: 130, Time elapsed: 2:20:05.699475\n",
      "User id: 894, Time elapsed: 2:20:36.698089\n",
      "User id: 900, Time elapsed: 2:22:33.439206\n",
      "User id: 901, Time elapsed: 2:24:22.215178\n",
      "User id: 903, Time elapsed: 2:26:17.754990\n",
      "User id: 648, Time elapsed: 2:28:13.100760\n",
      "User id: 137, Time elapsed: 2 days, 16:59:57.600737\n",
      "User id: 650, Time elapsed: 2 days, 17:01:44.849437\n",
      "User id: 392, Time elapsed: 2 days, 17:03:50.537400\n",
      "User id: 904, Time elapsed: 2 days, 17:05:37.143929\n",
      "User id: 394, Time elapsed: 2 days, 17:07:28.611399\n",
      "User id: 655, Time elapsed: 2 days, 17:09:02.465687\n",
      "User id: 660, Time elapsed: 2 days, 17:10:46.945522\n",
      "User id: 151, Time elapsed: 2 days, 17:12:25.282851\n",
      "User id: 919, Time elapsed: 2 days, 17:14:14.259851\n",
      "User id: 159, Time elapsed: 2 days, 17:16:02.263049\n",
      "User id: 416, Time elapsed: 2 days, 17:17:22.743904\n",
      "User id: 417, Time elapsed: 2 days, 17:19:07.142523\n",
      "User id: 674, Time elapsed: 2 days, 17:20:48.123890\n",
      "User id: 165, Time elapsed: 2 days, 17:22:29.331256\n",
      "User id: 934, Time elapsed: 2 days, 17:24:10.242676\n",
      "User id: 938, Time elapsed: 2 days, 17:25:50.785748\n",
      "User id: 427, Time elapsed: 2 days, 17:27:29.707941\n",
      "User id: 426, Time elapsed: 2 days, 17:29:08.436149\n",
      "User id: 685, Time elapsed: 2 days, 17:30:49.018890\n",
      "User id: 429, Time elapsed: 2 days, 17:31:52.396429\n",
      "User id: 686, Time elapsed: 2 days, 17:32:06.336622\n",
      "User id: 688, Time elapsed: 2 days, 17:33:45.555708\n",
      "User id: 689, Time elapsed: 2 days, 17:35:25.179194\n",
      "User id: 178, Time elapsed: 2 days, 17:37:03.004781\n",
      "User id: 177, Time elapsed: 2 days, 17:38:43.018332\n",
      "User id: 694, Time elapsed: 2 days, 17:39:01.247415\n",
      "User id: 439, Time elapsed: 2 days, 17:40:58.121907\n",
      "User id: 444, Time elapsed: 2 days, 17:42:48.648801\n",
      "User id: 194, Time elapsed: 2 days, 17:44:38.195019\n",
      "User id: 195, Time elapsed: 2 days, 17:46:25.376448\n",
      "User id: 450, Time elapsed: 2 days, 17:48:11.064950\n",
      "User id: 198, Time elapsed: 2 days, 17:50:02.346993\n",
      "User id: 711, Time elapsed: 2 days, 17:51:57.652357\n",
      "User id: 457, Time elapsed: 2 days, 17:53:14.830994\n",
      "User id: 458, Time elapsed: 2 days, 17:54:55.271330\n",
      "User id: 715, Time elapsed: 2 days, 17:56:41.935720\n",
      "User id: 205, Time elapsed: 2 days, 17:58:24.179805\n",
      "User id: 462, Time elapsed: 2 days, 18:00:06.933948\n",
      "User id: 721, Time elapsed: 2 days, 18:01:48.907460\n",
      "User id: 466, Time elapsed: 2 days, 18:03:31.190406\n",
      "User id: 210, Time elapsed: 2 days, 18:03:49.524128\n",
      "User id: 469, Time elapsed: 2 days, 18:04:08.083829\n",
      "User id: 214, Time elapsed: 2 days, 18:05:54.506919\n",
      "User id: 474, Time elapsed: 2 days, 18:06:50.876785\n",
      "User id: 219, Time elapsed: 2 days, 18:08:35.457942\n",
      "User id: 225, Time elapsed: 2 days, 18:10:20.439437\n",
      "User id: 739, Time elapsed: 2 days, 18:12:02.027091\n",
      "User id: 234, Time elapsed: 2 days, 18:12:36.154648\n",
      "User id: 237, Time elapsed: 2 days, 18:14:20.096554\n",
      "User id: 244, Time elapsed: 2 days, 18:16:02.950107\n",
      "User id: 764, Time elapsed: 2 days, 18:17:42.579862\n",
      "User id: 253, Time elapsed: 2 days, 18:19:22.758364\n",
      "User id: 254, Time elapsed: 2 days, 18:21:25.487998\n",
      "User id: 767, Time elapsed: 2 days, 18:21:46.477419\n"
     ]
    }
   ],
   "source": [
    "influence_top_n = defaultdict(list)\n",
    "influence_mae = defaultdict(list)\n",
    "\n",
    "all_users = set([trainset.to_raw_uid(iuid) for iuid in trainset.all_users()])\n",
    "no_mentors = set(np.random.choice(list(all_users.difference(all_mentors)), replace=False, size=100))\n",
    "\n",
    "starttime = dt.now()\n",
    "for ruid in no_mentors.union(all_mentors):\n",
    "    train_wo_df = train_df[train_df[\"user_id\"] != ruid]\n",
    "    raw_trainset_wo = [(ruid, riid, r, None) for ruid, riid, r in train_wo_df.to_records(index=False)]\n",
    "    trainset_wo = Dataset.construct_trainset(dataset, raw_trainset_wo)\n",
    "    \n",
    "    sim = UserKNN().compute_similarities(trainset_wo, min_support=1)\n",
    "    pop = UserKNN().compute_popularities(trainset_wo)\n",
    "    gain = UserKNN().compute_gain(trainset_wo)\n",
    "    \n",
    "    for k in Ks:\n",
    "        # UserKNN\n",
    "        if ruid in top_n_mentors[\"UserKNN\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"UserKNN\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"UserKNN\"]) < len(Ks):\n",
    "                influence_top_n[\"UserKNN\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"UserKNN\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"UserKNN\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "                \n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"UserKNN\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"UserKNN\"]) < len(Ks):\n",
    "                influence_mae[\"UserKNN\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"UserKNN\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"UserKNN\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "        # UserKNN + Reuse\n",
    "        if ruid in top_n_mentors[\"UserKNN + Reuse\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim, reuse=True)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"UserKNN + Reuse\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"UserKNN + Reuse\"]) < len(Ks):\n",
    "                influence_top_n[\"UserKNN + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"UserKNN + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"UserKNN + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "\n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"UserKNN + Reuse\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"UserKNN + Reuse\"]) < len(Ks):\n",
    "                influence_mae[\"UserKNN + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"UserKNN + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"UserKNN + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "        # Popularity\n",
    "        if ruid in top_n_mentors[\"Popularity\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"Popularity\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"Popularity\"]) < len(Ks):\n",
    "                influence_top_n[\"Popularity\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"Popularity\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"Popularity\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "\n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"Popularity\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"Popularity\"]) < len(Ks):\n",
    "                influence_mae[\"Popularity\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"Popularity\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"Popularity\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "        # Popularity + Reuse\n",
    "        if ruid in top_n_mentors[\"Popularity + Reuse\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5, reuse=True)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"Popularity + Reuse\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"Popularity + Reuse\"]) < len(Ks):\n",
    "                influence_top_n[\"Popularity + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"Popularity + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"Popularity + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "\n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"Popularity + Reuse\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"Popularity + Reuse\"]) < len(Ks):\n",
    "                influence_mae[\"Popularity + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"Popularity + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"Popularity + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "        # Gain\n",
    "        if ruid in top_n_mentors[\"Gain\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"Gain\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"Gain\"]) < len(Ks):\n",
    "                influence_top_n[\"Gain\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"Gain\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"Gain\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "\n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"Gain\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"Gain\"]) < len(Ks):\n",
    "                influence_mae[\"Gain\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"Gain\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"Gain\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "        # Gain + Reuse\n",
    "        if ruid in top_n_mentors[\"Gain + Reuse\"][Ks.index(k)] or ruid in no_mentors:\n",
    "            model = UserKNN(k=k, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5, reuse=True)\n",
    "            model.fit(trainset_wo)\n",
    "            predictions = model.test(testset)\n",
    "        \n",
    "            ruid_influence_top_n = measure_influence_top_n(base_top_n[\"Gain + Reuse\"][Ks.index(k)], get_top_n(predictions))\n",
    "            if len(influence_top_n[\"Gain + Reuse\"]) < len(Ks):\n",
    "                influence_top_n[\"Gain + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_top_n[\"Gain + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_top_n)]\n",
    "            else:\n",
    "                influence_top_n[\"Gain + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_top_n))\n",
    "\n",
    "            ruid_influence_mae = measure_influence_mae(base_mae[\"Gain + Reuse\"][Ks.index(k)], get_mae(predictions))\n",
    "            if len(influence_mae[\"Gain + Reuse\"]) < len(Ks):\n",
    "                influence_mae[\"Gain + Reuse\"] = [[] for _ in Ks]\n",
    "                influence_mae[\"Gain + Reuse\"][Ks.index(k)] = [(ruid, ruid_influence_mae)]\n",
    "            else:\n",
    "                influence_mae[\"Gain + Reuse\"][Ks.index(k)].append((ruid, ruid_influence_mae))\n",
    "        \n",
    "    print(\"User id: %d, Time elapsed: %s\" % (ruid, dt.now() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002654839122794252, 0.001636723241004774)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_mentors = []\n",
    "influence_nomentors = []\n",
    "for ruid, i in influence_mae[\"UserKNN\"][Ks.index(5)]:\n",
    "    if ruid in top_n_mentors[\"UserKNN\"][Ks.index(5)]:\n",
    "        influence_mentors.append(i)\n",
    "    else:\n",
    "        influence_nomentors.append(i)\n",
    "np.mean(influence_mentors), np.mean(influence_nomentors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00033861781227871617, 5.731552720721594e-05)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_mentors = []\n",
    "influence_nomentors = []\n",
    "for ruid, i in influence_mae[\"Gain + Reuse\"][Ks.index(5)]:\n",
    "    if ruid in top_n_mentors[\"Gain + Reuse\"][Ks.index(5)]:\n",
    "        influence_mentors.append(i)\n",
    "    else:\n",
    "        influence_nomentors.append(i)\n",
    "np.mean(influence_mentors), np.mean(influence_nomentors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_mae = pickle.load(open(\"results/ml-100k/influence/influence_mae.pkl\", \"rb\"))\n",
    "top_n_mentors = pickle.load(open(\"results/ml-100k/influence/top_10_mentors.pkl\", \"rb\"))\n",
    "influence_top_n = pickle.load(open(\"results/ml-100k/influence/influence_top_n.pkl\", \"rb\"))  \n",
    "Ks = pickle.load(open(\"results/ml-100k/influence/k.pkl\", \"rb\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_mae(method, k):\n",
    "    influence_mentors = []\n",
    "    influence_nomentors = []\n",
    "    for ruid, i in influence_mae[method][Ks.index(k)]:\n",
    "        if ruid in top_n_mentors[method][Ks.index(k)]:\n",
    "            influence_mentors.append(i)\n",
    "        else:\n",
    "            influence_nomentors.append(i)\n",
    "            \n",
    "    return np.mean(influence_mentors), np.mean(influence_nomentors)\n",
    "\n",
    "def f_top_n(method, k):\n",
    "    influence_mentors = []\n",
    "    influence_nomentors = []\n",
    "    for ruid, i in influence_top_n[method][Ks.index(k)]:\n",
    "        if ruid in top_n_mentors[method][Ks.index(k)]:\n",
    "            influence_mentors.append(i)\n",
    "        else:\n",
    "            influence_nomentors.append(i)\n",
    "            \n",
    "    return np.mean(influence_mentors), np.mean(influence_nomentors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_influence_mentors_mae = defaultdict(list)\n",
    "avg_influence_nomentors_mae = defaultdict(list)\n",
    "\n",
    "for k in Ks:\n",
    "    influence_mentors, influence_nomentors = f_mae(\"UserKNN\", k=k)\n",
    "    avg_influence_mentors_mae[\"UserKNN\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"UserKNN\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_mae(\"UserKNN + Reuse\", k=k)\n",
    "    avg_influence_mentors_mae[\"UserKNN + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"UserKNN + Reuse\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_mae(\"Popularity\", k=k)\n",
    "    avg_influence_mentors_mae[\"Popularity\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"Popularity\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_mae(\"Popularity + Reuse\", k=k)\n",
    "    avg_influence_mentors_mae[\"Popularity + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"Popularity + Reuse\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_mae(\"Gain\", k=k)\n",
    "    avg_influence_mentors_mae[\"Gain\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"Gain\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_mae(\"Gain + Reuse\", k=k)\n",
    "    avg_influence_mentors_mae[\"Gain + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_mae[\"Gain + Reuse\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    \n",
    "avg_influence_mentors_top_n = defaultdict(list)\n",
    "avg_influence_nomentors_top_n = defaultdict(list)\n",
    "\n",
    "for k in Ks:\n",
    "    influence_mentors, influence_nomentors = f_top_n(\"UserKNN\", k=k)\n",
    "    avg_influence_mentors_top_n[\"UserKNN\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"UserKNN\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_top_n(\"UserKNN + Reuse\", k=k)\n",
    "    avg_influence_mentors_top_n[\"UserKNN + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"UserKNN + Reuse\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_top_n(\"Popularity\", k=k)\n",
    "    avg_influence_mentors_top_n[\"Popularity\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"Popularity\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_top_n(\"Popularity + Reuse\", k=k)\n",
    "    avg_influence_mentors_top_n[\"Popularity + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"Popularity + Reuse\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_top_n(\"Gain\", k=k)\n",
    "    avg_influence_mentors_top_n[\"Gain\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"Gain\"].append(np.mean(influence_nomentors))\n",
    "    \n",
    "    influence_mentors, influence_nomentors = f_top_n(\"Gain + Reuse\", k=k)\n",
    "    avg_influence_mentors_top_n[\"Gain + Reuse\"].append(np.mean(influence_mentors))\n",
    "    avg_influence_nomentors_top_n[\"Gain + Reuse\"].append(np.mean(influence_nomentors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15, 30]\n"
     ]
    }
   ],
   "source": [
    "print(Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "barWidth = 0.1\n",
    " \n",
    "bars1 = avg_influence_mentors_mae[\"UserKNN\"]\n",
    "bars2 = avg_influence_mentors_mae[\"UserKNN + Reuse\"]\n",
    "bars3 = avg_influence_mentors_mae[\"Popularity\"]\n",
    "bars4 = avg_influence_mentors_mae[\"Popularity + Reuse\"]\n",
    "bars5 = avg_influence_mentors_mae[\"Gain\"]\n",
    "bars6 = avg_influence_mentors_mae[\"Gain + Reuse\"]\n",
    " \n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'C0', alpha=0.5, label='UserKNN')\n",
    "plt.bar(r3, bars3, width = barWidth, color = 'C1', alpha=0.5, label='Popularity')\n",
    "plt.bar(r5, bars5, width = barWidth, color = 'C2', alpha=0.5, label='Gain')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'C0', label='UserKNN + Reuse')\n",
    "plt.bar(r4, bars4, width = barWidth, color = 'C1', label='Popularity + Reuse')\n",
    "plt.bar(r6, bars6, width = barWidth, color = 'C2', label='Gain + Reuse')\n",
    "\n",
    "# general layout\n",
    "plt.axhline(y=0, linestyle=\"dashed\", color=\"grey\")\n",
    "plt.xticks(r3 + np.array(barWidth/2), [r\"$k=5$\", r\"$k=10$\", r\"$k=15$\", r\"$k=30$\"])\n",
    "plt.ylabel(\"Change in MAE\")\n",
    "plt.legend(ncol=2)\n",
    "plt.title(\"Influence of top-10 mentors\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "barWidth = 0.1\n",
    " \n",
    "bars1 = avg_influence_nomentors_mae[\"UserKNN\"]\n",
    "bars2 = avg_influence_nomentors_mae[\"UserKNN + Reuse\"]\n",
    "bars3 = avg_influence_nomentors_mae[\"Popularity\"]\n",
    "bars4 = avg_influence_nomentors_mae[\"Popularity + Reuse\"]\n",
    "bars5 = avg_influence_nomentors_mae[\"Gain\"]\n",
    "bars6 = avg_influence_nomentors_mae[\"Gain + Reuse\"]\n",
    " \n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'C0', alpha=0.5, label='UserKNN')\n",
    "plt.bar(r3, bars3, width = barWidth, color = 'C1', alpha=0.5, label='Popularity')\n",
    "plt.bar(r5, bars5, width = barWidth, color = 'C2', alpha=0.5, label='Gain')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'C0', label='UserKNN + Reuse')\n",
    "plt.bar(r4, bars4, width = barWidth, color = 'C1', label='Popularity + Reuse')\n",
    "plt.bar(r6, bars6, width = barWidth, color = 'C2', label='Gain + Reuse')\n",
    "\n",
    "# general layout\n",
    "plt.axhline(y=0, linestyle=\"dashed\", color=\"grey\")\n",
    "plt.xticks(r3 + np.array(barWidth/2), [r\"$k=5$\", r\"$k=10$\", r\"$k=15$\", r\"$k=30$\"])\n",
    "plt.ylabel(\"Change in MAE\")\n",
    "plt.legend(ncol=2)\n",
    "plt.title(\"Influence of all users\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "barWidth = 0.1\n",
    " \n",
    "bars1 = avg_influence_mentors_top_n[\"UserKNN\"]\n",
    "bars2 = avg_influence_mentors_top_n[\"UserKNN + Reuse\"]\n",
    "bars3 = avg_influence_mentors_top_n[\"Popularity\"]\n",
    "bars4 = avg_influence_mentors_top_n[\"Popularity + Reuse\"]\n",
    "bars5 = avg_influence_mentors_top_n[\"Gain\"]\n",
    "bars6 = avg_influence_mentors_top_n[\"Gain + Reuse\"]\n",
    " \n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'C0', alpha=0.5, label='UserKNN')\n",
    "plt.bar(r3, bars3, width = barWidth, color = 'C1', alpha=0.5, label='Popularity')\n",
    "plt.bar(r5, bars5, width = barWidth, color = 'C2', alpha=0.5, label='Gain')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'C0', label='UserKNN + Reuse')\n",
    "plt.bar(r4, bars4, width = barWidth, color = 'C1', label='Popularity + Reuse')\n",
    "plt.bar(r6, bars6, width = barWidth, color = 'C2', label='Gain + Reuse')\n",
    "\n",
    "# general layout\n",
    "plt.axhline(y=0, linestyle=\"dashed\", color=\"grey\")\n",
    "plt.xticks(r3 + np.array(barWidth/2), [r\"$k=5$\", r\"$k=10$\", r\"$k=15$\", r\"$k=30$\"])\n",
    "plt.ylabel(\"Jaccard Distance\")\n",
    "plt.legend(ncol=2)\n",
    "plt.title(\"Influence of top-10 mentors\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "barWidth = 0.1\n",
    " \n",
    "bars1 = avg_influence_nomentors_top_n[\"UserKNN\"]\n",
    "bars2 = avg_influence_nomentors_top_n[\"UserKNN + Reuse\"]\n",
    "bars3 = avg_influence_nomentors_top_n[\"Popularity\"]\n",
    "bars4 = avg_influence_nomentors_top_n[\"Popularity + Reuse\"]\n",
    "bars5 = avg_influence_nomentors_top_n[\"Gain\"]\n",
    "bars6 = avg_influence_nomentors_top_n[\"Gain + Reuse\"]\n",
    " \n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'C0', alpha=0.5, label='UserKNN')\n",
    "plt.bar(r3, bars3, width = barWidth, color = 'C1', alpha=0.5, label='Popularity')\n",
    "plt.bar(r5, bars5, width = barWidth, color = 'C2', alpha=0.5, label='Gain')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'C0', label='UserKNN + Reuse')\n",
    "plt.bar(r4, bars4, width = barWidth, color = 'C1', label='Popularity + Reuse')\n",
    "plt.bar(r6, bars6, width = barWidth, color = 'C2', label='Gain + Reuse')\n",
    "\n",
    "# general layout\n",
    "plt.axhline(y=0, linestyle=\"dashed\", color=\"grey\")\n",
    "plt.xticks(r3 + np.array(barWidth/2), [r\"$k=5$\", r\"$k=10$\", r\"$k=15$\", r\"$k=30$\"])\n",
    "plt.ylabel(\"Jaccard Distance\")\n",
    "plt.legend(ncol=2)\n",
    "plt.title(\"Influence of all users\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
