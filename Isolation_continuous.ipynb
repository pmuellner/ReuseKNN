{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmuellner\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\pmuellner\\Desktop\\DataLeakageKNN\\algorithms\\knn_neighborhood.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    }
   ],
   "source": [
    "import pyximport\n",
    "import numpy as np\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()},\n",
    "                  reload_support=True)\n",
    "from algorithms.knn_neighborhood import UserKNN\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime as dt\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(model):\n",
    "    n_user_ratings = {uid: len(ratings) for uid, ratings in model.trainset.ur.items()}\n",
    "    avg_item_ratings = defaultdict(list)\n",
    "    for uid, iid, r in trainset.all_ratings():\n",
    "        avg_item_ratings[iid].append(r)\n",
    "    for iid, ratings in avg_item_ratings.items():\n",
    "        avg_item_ratings[iid] = np.mean(ratings)\n",
    "    \n",
    "    distances = np.ones((model.trainset.n_users, model.trainset.n_users)) * np.inf\n",
    "    for alice in model.trainset.all_users():\n",
    "        d = defaultdict(list)\n",
    "        for bob, iid, r_bob, r_avg in model.known_ratings[alice]:\n",
    "            d[bob].append((iid, r_bob, r_avg))\n",
    "        \n",
    "        for bob, secrets in d.items():\n",
    "            known_iids = set([iid for iid, _, _ in secrets])\n",
    "            distance = 0\n",
    "            for iid, r_bob, r_avg in secrets:\n",
    "                distance += (r_bob - r_avg) ** 2\n",
    "            #unknown_iids = set([iid for iid, _ in model.trainset.ur[bob]]).difference(known_iids)\n",
    "            #for iid in unknown_iids:\n",
    "            #    distance += (avg_item_ratings[iid] - r_avg) ** 2\n",
    "            #distance = np.sqrt(distance)\n",
    "            \n",
    "            distances[alice, bob] = distance / len(secrets)\n",
    "            \n",
    "    return distances\n",
    "    \n",
    "def get_avg_distance(model):\n",
    "    D = get_distance_matrix(model)\n",
    "    \n",
    "    min_distances = np.amin(D, axis=0)\n",
    "    avg_distance = np.mean(min_distances[~np.isinf(min_distances)])\n",
    "    return avg_distance\n",
    "\n",
    "def get_avg_top_neighbor_distance(model):\n",
    "    n_known_secrets_matrix = np.zeros((trainset.n_users, trainset.n_users))\n",
    "    for alice in model.trainset.all_users():\n",
    "        for bob, secret in model.known_secrets[alice]:\n",
    "            n_known_secrets_matrix[alice, bob] += 1\n",
    "\n",
    "    top_neighbors = []\n",
    "    for alice, row in enumerate(n_known_secrets_matrix):\n",
    "        top_neighbor = np.argmax(row)\n",
    "        if n_known_secrets_matrix[alice, top_neighbor] > 0:\n",
    "            top_neighbors.append((alice, top_neighbor))\n",
    "\n",
    "    D = get_distance_matrix(model)\n",
    "    top_neighbor_distance = []\n",
    "    for alice, top_neighbor in top_neighbors:\n",
    "        top_neighbor_distance.append(D[alice, top_neighbor])\n",
    "\n",
    "    return np.mean(top_neighbor_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/ml-100k/u.data\", sep=\"\\t\")\n",
    "data_df.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data_df.drop(columns=[\"timestamp\"], axis=1, inplace=True)\n",
    "data_df[\"user_id\"] = data_df[\"user_id\"].map({b: a for a, b in enumerate(data_df[\"user_id\"].unique())})\n",
    "data_df[\"item_id\"] = data_df[\"item_id\"].map({b: a for a, b in enumerate(data_df[\"item_id\"].unique())})\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "dataset = Dataset.load_from_df(data_df, reader=reader)\n",
    "#trainset, testset = train_test_split(dataset, test_size=0.2)\n",
    "folds = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "25\n",
      "27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-5d105ae1bee9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUserKNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0muserknn_fold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_avg_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\DataLeakageKNN\\algorithms\\knn_neighborhood.pyx\u001b[0m in \u001b[0;36malgorithms.knn_neighborhood.UserKNN.fit\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Tradeoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0msimrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\surprise\\trainset.py\u001b[0m in \u001b[0;36mall_users\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mInner\u001b[0m \u001b[0mid\u001b[0m \u001b[0mof\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mall_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "userknn, userknnreuse, pop, popreuse, gain, gainreuse = [], [], [], [], [], []\n",
    "userknntop, userknnreusetop, poptop, popreusetop, gaintop, gainreusetop = [], [], [], [], [], []\n",
    "Ks = np.arange(1, 31, 2)\n",
    "for trainset, testset in folds.split(dataset):    \n",
    "    userknn_fold, userknnreuse_fold, pop_fold, popreuse_fold, gain_fold, gainreuse_fold = [], [], [], [], [], []\n",
    "    userknntop_fold, userknnreusetop_fold, poptop_fold, popreusetop_fold, gaintop_fold, gainreusetop_fold = [], [], [], [], [], []\n",
    "    for k in Ks:\n",
    "        print(k)\n",
    "        model = UserKNN(k=k)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        userknn_fold.append(get_avg_distance(model))\n",
    "        userknntop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "\n",
    "        model = UserKNN(k=k, reuse=True)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        userknnreuse_fold.append(get_avg_distance(model))\n",
    "        userknnreusetop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "\n",
    "        model = UserKNN(k=k, tau_2=0.5)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        pop_fold.append(get_avg_distance(model))\n",
    "        poptop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "\n",
    "        model = UserKNN(k=k, tau_2=0.5, reuse=True)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        popreuse_fold.append(get_avg_distance(model))\n",
    "        popreusetop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "\n",
    "        model = UserKNN(k=k, tau_4=0.5)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        gain_fold.append(get_avg_distance(model))\n",
    "        gaintop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "\n",
    "        model = UserKNN(k=k, tau_4=0.5, reuse=True)\n",
    "        model.fit(trainset)\n",
    "        _ = model.test(testset)\n",
    "        gainreuse_fold.append(get_avg_distance(model))\n",
    "        gainreusetop_fold.append(get_avg_top_neighbor_distance(model))\n",
    "    \n",
    "    userknn.append(userknn_fold)\n",
    "    userknnreuse.append(userknnreuse_fold)\n",
    "    pop.append(pop_fold)\n",
    "    popreuse.append(popreuse_fold)\n",
    "    gain.append(gain_fold)\n",
    "    gainreuse.append(gainreuse_fold)\n",
    "    \n",
    "    userknntop.append(userknntop_fold)\n",
    "    userknnreusetop.append(userknnreusetop_fold)\n",
    "    poptop.append(poptop_fold)\n",
    "    popreusetop.append(popreusetop_fold)\n",
    "    gaintop.append(gaintop_fold)\n",
    "    gainreusetop.append(gainreusetop_fold)\n",
    "    \n",
    "    print(\"fold finished\")\n",
    "    break\n",
    "    \n",
    "userknn = np.mean(userknn, axis=0)\n",
    "userknnreuse = np.mean(userknnreuse, axis=0)\n",
    "pop = np.mean(pop, axis=0)\n",
    "popreuse = np.mean(popreuse, axis=0)\n",
    "gain = np.mean(gain, axis=0)\n",
    "gainreuse = np.mean(gainreuse, axis=0)\n",
    "\n",
    "userknntop = np.mean(userknntop, axis=0)\n",
    "userknnreusetop = np.mean(userknnreusetop, axis=0)\n",
    "poptop = np.mean(poptop, axis=0)\n",
    "popreusetop = np.mean(popreusetop, axis=0)\n",
    "gaintop = np.mean(gaintop, axis=0)\n",
    "gainreusetop = np.mean(gainreusetop, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Ks, userknn, color=\"C0\", linestyle=\"dashed\", label=\"UserKNN\", alpha=0.5)\n",
    "plt.plot(Ks, pop, color=\"C1\", linestyle=\"dashed\", label=\"Popularity\", alpha=0.5)\n",
    "plt.plot(Ks, gain, color=\"C2\", linestyle=\"dashed\", label=\"Gain\", alpha=0.5)\n",
    "plt.plot(Ks, userknnreuse, color=\"C0\", linestyle=\"solid\", label=\"UserKNN + Reuse\")\n",
    "plt.plot(Ks, popreuse, color=\"C1\", linestyle=\"solid\", label=\"Popularity + Reuse\")\n",
    "plt.plot(Ks, gainreuse, color=\"C2\", linestyle=\"solid\", label=\"Gain + Reuse\")\n",
    "plt.xlabel(\"Nr. of neighbors\")\n",
    "plt.ylabel(\"Avg. (c, t)-isolation\")\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Ks, userknntop, color=\"C0\", linestyle=\"dashed\", label=\"UserKNN\", alpha=0.5)\n",
    "plt.plot(Ks, poptop, color=\"C1\", linestyle=\"dashed\", label=\"Popularity\", alpha=0.5)\n",
    "plt.plot(Ks, gaintop, color=\"C2\", linestyle=\"dashed\", label=\"Gain\", alpha=0.5)\n",
    "plt.plot(Ks, userknnreusetop, color=\"C0\", linestyle=\"solid\", label=\"UserKNN + Reuse\")\n",
    "plt.plot(Ks, popreusetop, color=\"C1\", linestyle=\"solid\", label=\"Popularity + Reuse\")\n",
    "plt.plot(Ks, gainreusetop, color=\"C2\", linestyle=\"solid\", label=\"Gain + Reuse\")\n",
    "plt.xlabel(\"Nr. of neighbors\")\n",
    "plt.ylabel(\"Avg. (c, t)-isolation of top neighbors\")\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "model = UserKNN(k=k)\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "\n",
    "deltas = []\n",
    "for u in model.trainset.all_users():\n",
    "    ranks = np.zeros((model.trainset.n_users))\n",
    "    simrank = {v: k for k, v in dict(enumerate(np.argsort(model.sim[u, :]))).items()}\n",
    "    for u_ in model.trainset.all_users():\n",
    "        ranks[u_] = simrank[u_]\n",
    "    max_rank = np.sort(ranks)[-1]\n",
    "    kth_rank = np.sort(ranks)[::-1][k-1]\n",
    "    deltas.append(max_rank - kth_rank)\n",
    "np.mean(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 44.   83.  107.  120.5 123.  128.  130.5 145.  146.5 152.  154.5 156.5\n",
      " 156.5 156.5 158.  162.5 164.  167.  172.5 177.  177.  183.5 184.  186.\n",
      " 186.  189.  189.5 191.5 192.5 192.5 195.5 196.  197.  198.  199.5 200.\n",
      " 200.5 202.5 204.  207.5 211.  212.  212.  213.  213.5 216.5 217.5 217.5\n",
      " 219.5 222.5 222.5 224.  226.  226.5 228.  228.5 228.5 228.5 229.  229.5\n",
      " 229.5 230.  230.5 231.  231.5 232.5 234.  234.  234.  236.  238.  241.\n",
      " 243.  243.  244.  245.  245.5 246.5 247.  249.5 251.5 251.5 252.  252.\n",
      " 252.5 253.5 254.  256.  256.5 257.  257.  257.5 257.5 258.5 259.  260.\n",
      " 260.  261.5 261.5 264.5 265.  265.  265.5 266.5 267.  268.  268.  268.5\n",
      " 268.5 268.5 269.5 270.  270.  270.  270.5 271.5 271.5 271.5 271.5 272.5\n",
      " 273.5 273.5 274.  276.  277.5 277.5 277.5 278.5 279.5 280.5 281.  281.5\n",
      " 281.5 282.5 282.5 282.5 282.5 283.  283.  283.  286.  286.  286.5 287.\n",
      " 288.  289.  289.  289.  289.5 290.  290.  290.  290.  291.  292.5 293.\n",
      " 293.5 294.  294.  294.5 294.5 294.5 295.  295.  295.  296.5 296.5 297.\n",
      " 298.  298.  299.  300.5 301.  301.5 301.5 301.5 303.  304.5 304.5 305.\n",
      " 305.  305.  305.5 307.5 308.5 309.  309.5 309.5 309.5 310.  310.  310.\n",
      " 310.5 310.5 310.5 312.  313.  313.  313.5 314.  314.  314.5 315.  315.5\n",
      " 316.  317.  317.  317.5 317.5 318.  318.  319.5 320.  321.  321.5 322.\n",
      " 323.  323.  323.5 323.5 324.  324.  324.  324.5 324.5 324.5 324.5 325.5\n",
      " 328.  328.5 329.  329.5 330.  331.5 331.5 332.  332.5 332.5 332.5 333.\n",
      " 333.  333.  334.  334.5 334.5 336.  336.5 337.5 337.5 338.  338.5 338.5\n",
      " 338.5 338.5 338.5 339.5 339.5 340.5 340.5 340.5 341.  341.  342.5 342.5\n",
      " 343.  343.  343.  344.  344.5 345.  345.5 346.5 349.  349.5 350.5 350.5\n",
      " 351.  352.  352.  352.  353.  353.  354.5 354.5 354.5 355.5 356.5 357.\n",
      " 357.  358.5 359.  359.5 359.5 360.5 360.5 361.  361.5 362.  363.  365.\n",
      " 365.5 365.5 366.5 366.5 367.  367.  367.  367.5 368.  368.5 369.  369.\n",
      " 369.5 370.  370.  370.5 370.5 371.  371.  371.5 371.5 371.5 373.5 373.5\n",
      " 374.  374.  374.5 375.  375.  375.  376.  376.  376.5 376.5 376.5 377.\n",
      " 377.5 377.5 378.  378.5 378.5 379.  379.5 380.5 381.  381.  382.5 382.5\n",
      " 383.  383.  383.  383.5 383.5 384.5 385.  385.5 386.5 386.5 387.5 387.5\n",
      " 388.5 388.5 388.5 389.  389.5 389.5 390.  390.  392.  393.  393.  394.\n",
      " 394.5 394.5 394.5 395.  395.  395.  395.5 396.  396.5 397.5 398.  398.5\n",
      " 398.5 398.5 398.5 399.  400.  400.5 401.  401.  401.5 401.5 402.  402.5\n",
      " 403.  403.  403.5 403.5 404.5 404.5 405.  405.  405.  407.  407.  408.\n",
      " 408.  408.5 408.5 409.  409.5 409.5 410.5 411.  411.  412.5 412.5 412.5\n",
      " 413.  413.  413.5 413.5 413.5 415.5 416.  416.  417.  417.5 417.5 418.\n",
      " 418.5 418.5 419.  419.5 420.  420.5 421.  421.5 422.5 422.5 424.5 424.5\n",
      " 425.  426.5 427.  427.  427.  427.5 427.5 428.5 429.  429.5 430.  430.\n",
      " 430.  430.5 431.5 431.5 431.5 433.  433.5 433.5 435.  435.  435.5 435.5\n",
      " 436.5 437.5 437.5 438.  438.  438.5 438.5 439.  440.  440.  441.  441.5\n",
      " 442.5 443.5 444.  444.5 445.5 446.  446.  446.  446.5 447.  447.  447.\n",
      " 448.  448.  448.  448.  448.  449.  449.  450.5 451.  451.  451.5 451.5\n",
      " 452.  452.  452.  452.5 452.5 452.5 453.  454.  454.  455.  455.  455.5\n",
      " 456.  457.  457.  457.5 457.5 458.5 458.5 459.5 459.5 460.  460.  461.\n",
      " 461.5 462.  462.  463.  464.  464.5 465.5 466.  467.  467.  467.  468.\n",
      " 468.  468.5 470.5 471.  471.5 471.5 472.  472.  473.  475.5 476.  476.\n",
      " 476.5 477.  477.5 477.5 478.  478.  478.5 478.5 480.  480.5 480.5 480.5\n",
      " 481.5 481.5 481.5 482.  482.  482.5 482.5 483.  484.  485.  486.5 487.\n",
      " 488.5 489.  489.5 492.5 492.5 492.5 493.  493.  493.  494.5 494.5 495.\n",
      " 495.5 497.  497.  498.  498.5 500.5 502.  502.5 503.  503.5 504.5 505.\n",
      " 505.  505.  505.5 505.5 505.5 506.  506.  507.  510.  510.5 511.  511.\n",
      " 511.5 511.5 514.5 515.5 518.  518.5 518.5 518.5 519.5 519.5 521.5 521.5\n",
      " 521.5 523.5 523.5 523.5 524.  525.  526.  529.  529.5 533.  533.5 533.5\n",
      " 534.5 535.  536.  537.  537.  537.5 538.  538.5 540.  540.5 540.5 541.\n",
      " 541.  541.5 545.  547.  547.5 549.  550.  550.  550.  550.  551.  551.\n",
      " 551.  552.  553.  553.5 553.5 554.5 554.5 555.5 555.5 556.  557.5 558.\n",
      " 558.5 559.  559.  560.  560.5 560.5 563.5 564.  564.5 565.5 566.  567.5\n",
      " 569.  569.5 570.  571.  572.  572.5 575.  576.5 577.  578.  578.5 579.5\n",
      " 582.5 582.5 583.  583.  583.5 584.  585.5 586.  586.5 587.  590.  590.\n",
      " 591.5 594.  594.5 594.5 595.5 597.5 598.  599.  600.5 600.5 601.  602.\n",
      " 603.  605.5 606.5 606.5 607.5 608.5 609.5 611.  611.5 612.5 613.  613.5\n",
      " 614.  614.  615.5 616.  617.  617.5 619.5 619.5 623.5 624.  624.  624.5\n",
      " 625.5 628.  628.5 629.5 631.  631.  632.  633.  634.5 635.  635.  637.5\n",
      " 639.  640.  641.  641.5 643.  643.5 645.5 645.5 648.  649.  649.5 656.\n",
      " 657.  662.  666.  666.  669.5 670.5 671.  674.5 676.  676.5 677.  677.5\n",
      " 678.  678.  679.  680.  680.5 685.5 685.5 686.  686.5 688.5 688.5 689.\n",
      " 690.5 693.5 696.  697.  697.5 697.5 699.  700.  701.5 705.5 707.5 707.5\n",
      " 710.  710.5 711.  711.5 712.  714.  714.5 715.5 716.  716.5 716.5 718.\n",
      " 720.  722.  723.  723.5 733.  733.5 734.5 738.5 739.  740.5 740.5 743.\n",
      " 743.5 745.  745.  745.5 747.5 748.  748.  750.  751.5 753.  756.5 757.\n",
      " 760.  761.  761.5 766.5 767.  768.  770.5 770.5 771.  773.5 774.  775.\n",
      " 775.5 776.  779.  780.  782.  784.  784.5 785.  786.5 786.5 788.5 789.\n",
      " 789.5 791.  792.  793.  793.5 794.  796.  796.5 798.5 798.5 799.  801.\n",
      " 801.  801.5 802.5 803.  804.  804.  808.  808.5 809.5 811.5 813.  813.\n",
      " 813.5 815.5 816.  816.5 822.  822.5 823.  824.5 829.5 830.5 831.  837.\n",
      " 837.5 837.5 838.5 840.  840.5 844.  844.  846.5 849.5 853.5 853.5 853.5\n",
      " 854.  856.5 863.  863.5 866.5 866.5 867.  875.5 875.5 881.5 886.  887.5\n",
      " 888.  891.  891.5 893.  894.5 895.  897.  899.5 900.5 902.5 904.5 912.5\n",
      " 924.5 925.5 927.5 934.5 937.5 939.  941.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "model = UserKNN(k=k, tau_2=0.5)\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "\n",
    "deltas = []\n",
    "poprank = {v: k+1 for k, v in dict(enumerate(np.argsort(model.pop)[::-1])).items()}\n",
    "for u in model.trainset.all_users():\n",
    "    ranks = np.zeros((model.trainset.n_users))\n",
    "    simrank = {v: k+1 for k, v in dict(enumerate(np.argsort(model.sim[u, :])[::-1])).items()}\n",
    "    for u_ in model.trainset.all_users():\n",
    "        ranks[u_] = 0.5 * poprank[u_] + 0.5 * simrank[u_]\n",
    "    print(np.sort(ranks))\n",
    "    break\n",
    "    min_rank = np.sort(ranks)[0]\n",
    "    kth_rank = np.sort(ranks)[k-1]\n",
    "    deltas.append(max_rank - kth_rank)\n",
    "np.mean(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 708.5,\n",
       " 1: 603.5,\n",
       " 2: 665.5,\n",
       " 3: 655.0,\n",
       " 4: 595.5,\n",
       " 5: 304.0,\n",
       " 6: 262.5,\n",
       " 7: 614.0,\n",
       " 8: 435.5,\n",
       " 9: 691.5,\n",
       " 10: 726.0,\n",
       " 11: 615.5,\n",
       " 12: 734.0,\n",
       " 13: 463.0,\n",
       " 14: 477.5,\n",
       " 15: 670.5,\n",
       " 16: 601.0,\n",
       " 17: 724.5,\n",
       " 18: 683.0,\n",
       " 19: 712.0,\n",
       " 20: 717.5,\n",
       " 21: 598.0,\n",
       " 22: 552.5,\n",
       " 23: 256.0,\n",
       " 24: 560.5,\n",
       " 25: 591.5,\n",
       " 26: 771.0,\n",
       " 27: 674.0,\n",
       " 28: 600.0,\n",
       " 29: 651.0,\n",
       " 30: 554.0,\n",
       " 31: 503.5,\n",
       " 32: 722.5,\n",
       " 33: 523.5,\n",
       " 34: 644.0,\n",
       " 35: 500.0,\n",
       " 36: 202.0,\n",
       " 37: 649.5,\n",
       " 38: 739.5,\n",
       " 39: 459.0,\n",
       " 40: 630.0,\n",
       " 41: 639.0,\n",
       " 42: 521.0,\n",
       " 43: 717.5,\n",
       " 44: 748.5,\n",
       " 45: 540.0,\n",
       " 46: 800.5,\n",
       " 47: 679.0,\n",
       " 48: 593.5,\n",
       " 49: 690.5,\n",
       " 50: 661.5,\n",
       " 51: 535.0,\n",
       " 52: 590.5,\n",
       " 53: 731.0,\n",
       " 54: 620.5,\n",
       " 55: 603.5,\n",
       " 56: 595.5,\n",
       " 57: 558.5,\n",
       " 58: 490.5,\n",
       " 59: 513.0,\n",
       " 60: 412.5,\n",
       " 61: 515.0,\n",
       " 62: 486.0,\n",
       " 63: 178.5,\n",
       " 64: 621.5,\n",
       " 65: 597.0,\n",
       " 66: 603.5,\n",
       " 67: 678.5,\n",
       " 68: 629.0,\n",
       " 69: 138.5,\n",
       " 70: 120.5,\n",
       " 71: 737.5,\n",
       " 72: 603.5,\n",
       " 73: 744.5,\n",
       " 74: 644.5,\n",
       " 75: 587.5,\n",
       " 76: 613.0,\n",
       " 77: 631.0,\n",
       " 78: 668.5,\n",
       " 79: 582.5,\n",
       " 80: 679.5,\n",
       " 81: 773.5,\n",
       " 82: 239.5,\n",
       " 83: 544.0,\n",
       " 84: 564.5,\n",
       " 85: 698.0,\n",
       " 86: 612.5,\n",
       " 87: 599.5,\n",
       " 88: 436.5,\n",
       " 89: 523.5,\n",
       " 90: 588.0,\n",
       " 91: 583.5,\n",
       " 92: 378.0,\n",
       " 93: 715.0,\n",
       " 94: 637.0,\n",
       " 95: 677.5,\n",
       " 96: 768.0,\n",
       " 97: 744.0,\n",
       " 98: 531.5,\n",
       " 99: 626.0,\n",
       " 100: 658.5,\n",
       " 101: 731.5,\n",
       " 102: 272.0,\n",
       " 103: 659.0,\n",
       " 104: 232.0,\n",
       " 105: 130.0,\n",
       " 106: 548.0,\n",
       " 107: 672.0,\n",
       " 108: 738.0,\n",
       " 109: 737.5,\n",
       " 110: 590.5,\n",
       " 111: 575.0,\n",
       " 112: 622.5,\n",
       " 113: 618.5,\n",
       " 114: 578.0,\n",
       " 115: 792.0,\n",
       " 116: 655.0,\n",
       " 117: 775.5,\n",
       " 118: 781.5,\n",
       " 119: 686.0,\n",
       " 120: 716.5,\n",
       " 121: 637.0,\n",
       " 122: 744.5,\n",
       " 123: 232.0,\n",
       " 124: 719.5,\n",
       " 125: 676.0,\n",
       " 126: 339.5,\n",
       " 127: 532.0,\n",
       " 128: 673.0,\n",
       " 129: 641.0,\n",
       " 130: 684.5,\n",
       " 131: 465.5,\n",
       " 132: 565.0,\n",
       " 133: 216.0,\n",
       " 134: 727.5,\n",
       " 135: 334.5,\n",
       " 136: 659.0,\n",
       " 137: 688.5,\n",
       " 138: 718.5,\n",
       " 139: 627.5,\n",
       " 140: 477.0,\n",
       " 141: 653.5,\n",
       " 142: 672.0,\n",
       " 143: 601.5,\n",
       " 144: 602.0,\n",
       " 145: 595.0,\n",
       " 146: 535.5,\n",
       " 147: 553.0,\n",
       " 148: 623.0,\n",
       " 149: 675.0,\n",
       " 150: 663.0,\n",
       " 151: 603.0,\n",
       " 152: 505.0,\n",
       " 153: 560.0,\n",
       " 154: 775.0,\n",
       " 155: 306.0,\n",
       " 156: 504.5,\n",
       " 157: 537.5,\n",
       " 158: 778.0,\n",
       " 159: 714.5,\n",
       " 160: 582.0,\n",
       " 161: 428.0,\n",
       " 162: 246.5,\n",
       " 163: 538.5,\n",
       " 164: 566.5,\n",
       " 165: 661.0,\n",
       " 166: 805.0,\n",
       " 167: 739.0,\n",
       " 168: 595.5,\n",
       " 169: 658.0,\n",
       " 170: 774.0,\n",
       " 171: 551.5,\n",
       " 172: 612.5,\n",
       " 173: 651.0,\n",
       " 174: 483.0,\n",
       " 175: 660.5,\n",
       " 176: 704.0,\n",
       " 177: 546.0,\n",
       " 178: 645.0,\n",
       " 179: 580.0,\n",
       " 180: 740.5,\n",
       " 181: 551.0,\n",
       " 182: 593.5,\n",
       " 183: 167.5,\n",
       " 184: 140.5,\n",
       " 185: 627.5,\n",
       " 186: 619.5,\n",
       " 187: 662.0,\n",
       " 188: 459.0,\n",
       " 189: 125.5,\n",
       " 190: 717.0,\n",
       " 191: 744.0,\n",
       " 192: 578.0,\n",
       " 193: 583.0,\n",
       " 194: 729.0,\n",
       " 195: 164.0,\n",
       " 196: 744.5,\n",
       " 197: 414.0,\n",
       " 198: 760.5,\n",
       " 199: 699.0,\n",
       " 200: 613.0,\n",
       " 201: 760.5,\n",
       " 202: 455.0,\n",
       " 203: 295.0,\n",
       " 204: 182.0,\n",
       " 205: 559.5,\n",
       " 206: 677.5,\n",
       " 207: 93.0,\n",
       " 208: 745.5,\n",
       " 209: 616.5,\n",
       " 210: 718.0,\n",
       " 211: 641.0,\n",
       " 212: 703.5,\n",
       " 213: 536.5,\n",
       " 214: 689.0,\n",
       " 215: 593.5,\n",
       " 216: 540.5,\n",
       " 217: 594.0,\n",
       " 218: 211.0,\n",
       " 219: 603.0,\n",
       " 220: 686.5,\n",
       " 221: 341.5,\n",
       " 222: 734.0,\n",
       " 223: 497.5,\n",
       " 224: 684.5,\n",
       " 225: 705.0,\n",
       " 226: 541.0,\n",
       " 227: 702.0,\n",
       " 228: 523.5,\n",
       " 229: 485.0,\n",
       " 230: 646.0,\n",
       " 231: 233.0,\n",
       " 232: 652.0,\n",
       " 233: 159.5,\n",
       " 234: 641.5,\n",
       " 235: 652.0,\n",
       " 236: 672.0,\n",
       " 237: 749.5,\n",
       " 238: 531.0,\n",
       " 239: 555.5,\n",
       " 240: 689.0,\n",
       " 241: 443.5,\n",
       " 242: 729.0,\n",
       " 243: 650.5,\n",
       " 244: 130.5,\n",
       " 245: 685.0,\n",
       " 246: 631.0,\n",
       " 247: 612.0,\n",
       " 248: 498.0,\n",
       " 249: 639.5,\n",
       " 250: 727.0,\n",
       " 251: 265.0,\n",
       " 252: 645.0,\n",
       " 253: 662.0,\n",
       " 254: 631.5,\n",
       " 255: 598.5,\n",
       " 256: 748.0,\n",
       " 257: 655.5,\n",
       " 258: 679.5,\n",
       " 259: 609.5,\n",
       " 260: 714.5,\n",
       " 261: 570.5,\n",
       " 262: 508.5,\n",
       " 263: 675.0,\n",
       " 264: 503.0,\n",
       " 265: 54.5,\n",
       " 266: 255.0,\n",
       " 267: 576.0,\n",
       " 268: 169.0,\n",
       " 269: 432.5,\n",
       " 270: 746.0,\n",
       " 271: 613.0,\n",
       " 272: 743.0,\n",
       " 273: 708.0,\n",
       " 274: 557.5,\n",
       " 275: 630.5,\n",
       " 276: 436.0,\n",
       " 277: 686.0,\n",
       " 278: 610.0,\n",
       " 279: 531.5,\n",
       " 280: 676.5,\n",
       " 281: 647.5,\n",
       " 282: 129.0,\n",
       " 283: 564.5,\n",
       " 284: 602.5,\n",
       " 285: 562.5,\n",
       " 286: 499.5,\n",
       " 287: 127.0,\n",
       " 288: 522.0,\n",
       " 289: 656.0,\n",
       " 290: 763.5,\n",
       " 291: 110.0,\n",
       " 292: 660.5,\n",
       " 293: 519.5,\n",
       " 294: 457.0,\n",
       " 295: 653.5,\n",
       " 296: 74.0,\n",
       " 297: 622.5,\n",
       " 298: 630.0,\n",
       " 299: 601.0,\n",
       " 300: 696.5,\n",
       " 301: 683.5,\n",
       " 302: 624.0,\n",
       " 303: 596.0,\n",
       " 304: 687.5,\n",
       " 305: 512.5,\n",
       " 306: 119.0,\n",
       " 307: 599.5,\n",
       " 308: 378.5,\n",
       " 309: 771.5,\n",
       " 310: 600.0,\n",
       " 311: 707.5,\n",
       " 312: 591.0,\n",
       " 313: 581.5,\n",
       " 314: 404.0,\n",
       " 315: 705.0,\n",
       " 316: 624.0,\n",
       " 317: 468.5,\n",
       " 318: 214.0,\n",
       " 319: 569.5,\n",
       " 320: 535.5,\n",
       " 321: 639.5,\n",
       " 322: 645.0,\n",
       " 323: 266.5,\n",
       " 324: 800.5,\n",
       " 325: 63.0,\n",
       " 326: 427.0,\n",
       " 327: 539.0,\n",
       " 328: 221.0,\n",
       " 329: 444.5,\n",
       " 330: 607.5,\n",
       " 331: 612.5,\n",
       " 332: 551.5,\n",
       " 333: 589.0,\n",
       " 334: 544.0,\n",
       " 335: 653.0,\n",
       " 336: 715.5,\n",
       " 337: 625.0,\n",
       " 338: 762.0,\n",
       " 339: 621.5,\n",
       " 340: 565.5,\n",
       " 341: 644.5,\n",
       " 342: 340.0,\n",
       " 343: 28.0,\n",
       " 344: 419.5,\n",
       " 345: 504.5,\n",
       " 346: 686.0,\n",
       " 347: 494.0,\n",
       " 348: 761.0,\n",
       " 349: 613.5,\n",
       " 350: 664.5,\n",
       " 351: 678.0,\n",
       " 352: 446.5,\n",
       " 353: 642.5,\n",
       " 354: 379.5,\n",
       " 355: 491.0,\n",
       " 356: 347.5,\n",
       " 357: 719.5,\n",
       " 358: 657.0,\n",
       " 359: 131.0,\n",
       " 360: 389.0,\n",
       " 361: 624.0,\n",
       " 362: 494.0,\n",
       " 363: 590.5,\n",
       " 364: 599.0,\n",
       " 365: 639.0,\n",
       " 366: 572.0,\n",
       " 367: 515.0,\n",
       " 368: 547.0,\n",
       " 369: 760.0,\n",
       " 370: 152.5,\n",
       " 371: 458.5,\n",
       " 372: 517.0,\n",
       " 373: 730.5,\n",
       " 374: 626.5,\n",
       " 375: 338.5,\n",
       " 376: 689.0,\n",
       " 377: 183.0,\n",
       " 378: 714.5,\n",
       " 379: 514.0,\n",
       " 380: 183.0,\n",
       " 381: 636.5,\n",
       " 382: 448.0,\n",
       " 383: 803.0,\n",
       " 384: 537.0,\n",
       " 385: 578.5,\n",
       " 386: 610.0,\n",
       " 387: 520.0,\n",
       " 388: 457.5,\n",
       " 389: 533.5,\n",
       " 390: 618.0,\n",
       " 391: 755.5,\n",
       " 392: 486.5,\n",
       " 393: 539.0,\n",
       " 394: 677.5,\n",
       " 395: 146.0,\n",
       " 396: 741.0,\n",
       " 397: 544.0,\n",
       " 398: 642.5,\n",
       " 399: 102.0,\n",
       " 400: 686.5,\n",
       " 401: 432.0,\n",
       " 402: 577.0,\n",
       " 403: 59.5,\n",
       " 404: 644.0,\n",
       " 405: 571.5,\n",
       " 406: 299.0,\n",
       " 407: 219.0,\n",
       " 408: 701.0,\n",
       " 409: 582.5,\n",
       " 410: 495.0,\n",
       " 411: 495.5,\n",
       " 412: 791.5,\n",
       " 413: 533.5,\n",
       " 414: 557.0,\n",
       " 415: 15.0,\n",
       " 416: 194.0,\n",
       " 417: 416.0,\n",
       " 418: 657.0,\n",
       " 419: 685.0,\n",
       " 420: 679.5,\n",
       " 421: 751.5,\n",
       " 422: 582.0,\n",
       " 423: 471.0,\n",
       " 424: 485.5,\n",
       " 425: 591.5,\n",
       " 426: 532.0,\n",
       " 427: 746.0,\n",
       " 428: 243.0,\n",
       " 429: 280.0,\n",
       " 430: 431.0,\n",
       " 431: 544.5,\n",
       " 432: 633.0,\n",
       " 433: 674.0,\n",
       " 434: 685.0,\n",
       " 435: 382.0,\n",
       " 436: 358.0,\n",
       " 437: 695.5,\n",
       " 438: 69.5,\n",
       " 439: 451.0,\n",
       " 440: 733.5,\n",
       " 441: 686.0,\n",
       " 442: 646.5,\n",
       " 443: 713.0,\n",
       " 444: 677.0,\n",
       " 445: 598.0,\n",
       " 446: 551.5,\n",
       " 447: 551.0,\n",
       " 448: 493.0,\n",
       " 449: 153.0,\n",
       " 450: 770.0,\n",
       " 451: 667.0,\n",
       " 452: 657.0,\n",
       " 453: 219.0,\n",
       " 454: 558.0,\n",
       " 455: 264.5,\n",
       " 456: 173.5,\n",
       " 457: 573.5,\n",
       " 458: 386.5,\n",
       " 459: 667.0,\n",
       " 460: 632.0,\n",
       " 461: 378.0,\n",
       " 462: 439.0,\n",
       " 463: 498.5,\n",
       " 464: 635.5,\n",
       " 465: 161.5,\n",
       " 466: 644.5,\n",
       " 467: 652.0,\n",
       " 468: 753.5,\n",
       " 469: 80.5,\n",
       " 470: 488.5,\n",
       " 471: 638.0,\n",
       " 472: 671.5,\n",
       " 473: 84.0,\n",
       " 474: 465.0,\n",
       " 475: 658.0,\n",
       " 476: 394.0,\n",
       " 477: 381.5,\n",
       " 478: 134.5,\n",
       " 479: 459.5,\n",
       " 480: 584.0,\n",
       " 481: 33.0,\n",
       " 482: 649.0,\n",
       " 483: 631.0,\n",
       " 484: 657.0,\n",
       " 485: 437.5,\n",
       " 486: 400.0,\n",
       " 487: 386.5,\n",
       " 488: 37.5,\n",
       " 489: 550.0,\n",
       " 490: 680.0,\n",
       " 491: 793.0,\n",
       " 492: 686.5,\n",
       " 493: 690.0,\n",
       " 494: 752.0,\n",
       " 495: 92.0,\n",
       " 496: 236.0,\n",
       " 497: 145.0,\n",
       " 498: 425.0,\n",
       " 499: 657.0,\n",
       " 500: 549.5,\n",
       " 501: 715.0,\n",
       " 502: 601.5,\n",
       " 503: 506.0,\n",
       " 504: 97.0,\n",
       " 505: 697.5,\n",
       " 506: 679.0,\n",
       " 507: 698.0,\n",
       " 508: 418.0,\n",
       " 509: 691.5,\n",
       " 510: 506.0,\n",
       " 511: 539.0,\n",
       " 512: 127.5,\n",
       " 513: 164.0,\n",
       " 514: 487.5,\n",
       " 515: 635.0,\n",
       " 516: 433.5,\n",
       " 517: 749.5,\n",
       " 518: 691.5,\n",
       " 519: 156.0,\n",
       " 520: 177.5,\n",
       " 521: 215.0,\n",
       " 522: 622.5,\n",
       " 523: 652.5,\n",
       " 524: 593.0,\n",
       " 525: 561.0,\n",
       " 526: 497.0,\n",
       " 527: 184.5,\n",
       " 528: 585.0,\n",
       " 529: 760.0,\n",
       " 530: 265.0,\n",
       " 531: 555.0,\n",
       " 532: 571.5,\n",
       " 533: 656.0,\n",
       " 534: 125.5,\n",
       " 535: 190.5,\n",
       " 536: 636.0,\n",
       " 537: 404.5,\n",
       " 538: 172.5,\n",
       " 539: 644.0,\n",
       " 540: 245.5,\n",
       " 541: 712.5,\n",
       " 542: 98.5,\n",
       " 543: 278.0,\n",
       " 544: 258.5,\n",
       " 545: 673.5,\n",
       " 546: 670.0,\n",
       " 547: 212.5,\n",
       " 548: 259.0,\n",
       " 549: 615.0,\n",
       " 550: 128.5,\n",
       " 551: 765.0,\n",
       " 552: 693.0,\n",
       " 553: 408.5,\n",
       " 554: 673.0,\n",
       " 555: 754.5,\n",
       " 556: 642.5,\n",
       " 557: 635.5,\n",
       " 558: 187.5,\n",
       " 559: 239.5,\n",
       " 560: 275.0,\n",
       " 561: 439.0,\n",
       " 562: 165.0,\n",
       " 563: 96.0,\n",
       " 564: 140.0,\n",
       " 565: 646.5,\n",
       " 566: 253.0,\n",
       " 567: 592.5,\n",
       " 568: 527.5,\n",
       " 569: 143.0,\n",
       " 570: 524.0,\n",
       " 571: 316.5,\n",
       " 572: 689.5,\n",
       " 573: 147.0,\n",
       " 574: 145.5,\n",
       " 575: 517.5,\n",
       " 576: 474.0,\n",
       " 577: 478.0,\n",
       " 578: 152.0,\n",
       " 579: 531.0,\n",
       " 580: 443.5,\n",
       " 581: 733.0,\n",
       " 582: 627.5,\n",
       " 583: 453.5,\n",
       " 584: 480.0,\n",
       " 585: 745.0,\n",
       " 586: 469.0,\n",
       " 587: 620.0,\n",
       " 588: 484.0,\n",
       " 589: 428.0,\n",
       " 590: 651.5,\n",
       " 591: 505.0,\n",
       " 592: 538.0,\n",
       " 593: 181.5,\n",
       " 594: 567.0,\n",
       " 595: 698.5,\n",
       " 596: 129.5,\n",
       " 597: 602.5,\n",
       " 598: 162.0,\n",
       " 599: 296.5,\n",
       " 600: 576.0,\n",
       " 601: 589.0,\n",
       " 602: 743.5,\n",
       " 603: 646.5,\n",
       " 604: 452.0,\n",
       " 605: 538.0,\n",
       " 606: 90.0,\n",
       " 607: 59.0,\n",
       " 608: 290.5,\n",
       " 609: 112.5,\n",
       " 610: 114.0,\n",
       " 611: 742.0,\n",
       " 612: 488.5,\n",
       " 613: 481.5,\n",
       " 614: 212.0,\n",
       " 615: 71.5,\n",
       " 616: 632.5,\n",
       " 617: 562.5,\n",
       " 618: 585.5,\n",
       " 619: 785.0,\n",
       " 620: 156.0,\n",
       " 621: 217.5,\n",
       " 622: 564.0,\n",
       " 623: 615.0,\n",
       " 624: 599.5,\n",
       " 625: 521.0,\n",
       " 626: 639.0,\n",
       " 627: 523.0,\n",
       " 628: 580.5,\n",
       " 629: 545.0,\n",
       " 630: 719.0,\n",
       " 631: 525.5,\n",
       " 632: 186.5,\n",
       " 633: 612.0,\n",
       " 634: 145.5,\n",
       " 635: 116.5,\n",
       " 636: 478.5,\n",
       " 637: 546.5,\n",
       " 638: 697.0,\n",
       " 639: 104.5,\n",
       " 640: 437.0,\n",
       " 641: 135.5,\n",
       " 642: 658.5,\n",
       " 643: 128.5,\n",
       " 644: 158.0,\n",
       " 645: 581.5,\n",
       " 646: 577.0,\n",
       " 647: 279.5,\n",
       " 648: 681.5,\n",
       " 649: 449.5,\n",
       " 650: 107.0,\n",
       " 651: 675.0,\n",
       " 652: 564.5,\n",
       " 653: 294.5,\n",
       " 654: 558.0,\n",
       " 655: 479.0,\n",
       " 656: 443.0,\n",
       " 657: 544.5,\n",
       " 658: 495.0,\n",
       " 659: 240.5,\n",
       " 660: 510.5,\n",
       " 661: 137.5,\n",
       " 662: 659.5,\n",
       " 663: 465.5,\n",
       " 664: 561.0,\n",
       " 665: 602.0,\n",
       " 666: 634.5,\n",
       " 667: 594.0,\n",
       " 668: 564.0,\n",
       " 669: 543.0,\n",
       " 670: 708.0,\n",
       " 671: 649.0,\n",
       " 672: 602.0,\n",
       " 673: 114.0,\n",
       " 674: 77.5,\n",
       " 675: 622.5,\n",
       " 676: 611.5,\n",
       " 677: 668.5,\n",
       " 678: 565.5,\n",
       " 679: 162.5,\n",
       " 680: 495.0,\n",
       " 681: 402.5,\n",
       " 682: 631.5,\n",
       " 683: 142.5,\n",
       " 684: 685.0,\n",
       " 685: 537.5,\n",
       " 686: 570.0,\n",
       " 687: 113.5,\n",
       " 688: 196.5,\n",
       " 689: 563.0,\n",
       " 690: 67.0,\n",
       " 691: 103.5,\n",
       " 692: 678.5,\n",
       " 693: 595.0,\n",
       " 694: 486.5,\n",
       " 695: 103.0,\n",
       " 696: 82.5,\n",
       " 697: 178.0,\n",
       " 698: 153.5,\n",
       " 699: 371.5,\n",
       " 700: 273.5,\n",
       " 701: 580.0,\n",
       " 702: 479.5,\n",
       " 703: 526.0,\n",
       " 704: 445.0,\n",
       " 705: 538.5,\n",
       " 706: 667.0,\n",
       " 707: 222.0,\n",
       " 708: 140.0,\n",
       " 709: 569.5,\n",
       " 710: 240.0,\n",
       " 711: 627.0,\n",
       " 712: 407.5,\n",
       " 713: 503.0,\n",
       " 714: 427.0,\n",
       " 715: 167.0,\n",
       " 716: 535.0,\n",
       " 717: 455.5,\n",
       " 718: 502.5,\n",
       " 719: 56.5,\n",
       " 720: 117.5,\n",
       " 721: 187.5,\n",
       " 722: 566.5,\n",
       " 723: 167.5,\n",
       " 724: 496.5,\n",
       " 725: 91.0,\n",
       " 726: 138.5,\n",
       " 727: 95.5,\n",
       " 728: 182.5,\n",
       " 729: 240.5,\n",
       " 730: 316.5,\n",
       " 731: 630.0,\n",
       " 732: 629.5,\n",
       " 733: 584.0,\n",
       " 734: 572.5,\n",
       " 735: 185.5,\n",
       " 736: 544.5,\n",
       " 737: 735.5,\n",
       " 738: 551.5,\n",
       " 739: 681.0,\n",
       " 740: 94.0,\n",
       " 741: 235.5,\n",
       " 742: 339.0,\n",
       " 743: 736.5,\n",
       " 744: 554.0,\n",
       " 745: 618.5,\n",
       " 746: 702.5,\n",
       " 747: 173.5,\n",
       " 748: 524.0,\n",
       " 749: 410.0,\n",
       " 750: 350.5,\n",
       " 751: 687.5,\n",
       " 752: 448.5,\n",
       " 753: 498.0,\n",
       " 754: 421.0,\n",
       " 755: 552.5,\n",
       " 756: 634.0,\n",
       " 757: 307.0,\n",
       " 758: 638.5,\n",
       " 759: 649.5,\n",
       " 760: 667.0,\n",
       " 761: 526.5,\n",
       " 762: 103.5,\n",
       " 763: 617.5,\n",
       " 764: 605.0,\n",
       " 765: 106.5,\n",
       " 766: 226.0,\n",
       " 767: 696.0,\n",
       " 768: 416.5,\n",
       " 769: 150.5,\n",
       " 770: 150.5,\n",
       " 771: 665.5,\n",
       " 772: 656.0,\n",
       " 773: 270.0,\n",
       " 774: 84.0,\n",
       " 775: 658.5,\n",
       " 776: 505.0,\n",
       " 777: 185.0,\n",
       " 778: 486.5,\n",
       " 779: 479.0,\n",
       " 780: 220.5,\n",
       " 781: 422.0,\n",
       " 782: 89.0,\n",
       " 783: 387.5,\n",
       " 784: 53.5,\n",
       " 785: 414.0,\n",
       " 786: 186.5,\n",
       " 787: 58.0,\n",
       " 788: 127.0,\n",
       " 789: 706.0,\n",
       " 790: 368.5,\n",
       " 791: 209.5,\n",
       " 792: 150.0,\n",
       " 793: 509.5,\n",
       " 794: 65.5,\n",
       " 795: 250.0,\n",
       " 796: 195.5,\n",
       " 797: 58.5,\n",
       " 798: 540.0,\n",
       " 799: 138.0,\n",
       " 800: 113.0,\n",
       " 801: 603.5,\n",
       " 802: 147.0,\n",
       " 803: 36.5,\n",
       " 804: 505.0,\n",
       " 805: 41.5,\n",
       " 806: 475.0,\n",
       " 807: 494.0,\n",
       " 808: 53.0,\n",
       " 809: 627.0,\n",
       " 810: 87.5,\n",
       " 811: 22.5,\n",
       " 812: 346.0,\n",
       " 813: 175.5,\n",
       " 814: 23.0,\n",
       " 815: 99.0,\n",
       " 816: 545.0,\n",
       " 817: 146.0,\n",
       " 818: 595.5,\n",
       " 819: 24.5,\n",
       " 820: 95.5,\n",
       " 821: 55.0,\n",
       " 822: 114.0,\n",
       " 823: 618.0,\n",
       " 824: 132.0,\n",
       " 825: 37.5,\n",
       " 826: 82.0,\n",
       " 827: 138.0,\n",
       " 828: 679.0,\n",
       " 829: 127.0,\n",
       " 830: 465.0,\n",
       " 831: 107.5,\n",
       " 832: 684.5,\n",
       " 833: 706.5,\n",
       " 834: 475.0,\n",
       " 835: 65.5,\n",
       " 836: 176.5,\n",
       " 837: 113.5,\n",
       " 838: 266.0,\n",
       " 839: 421.0,\n",
       " 840: 536.5,\n",
       " 841: 240.0,\n",
       " 842: 150.0,\n",
       " 843: 560.5,\n",
       " 844: 596.0,\n",
       " 845: 160.5,\n",
       " 846: 471.0,\n",
       " 847: 454.0,\n",
       " 848: 132.0,\n",
       " 849: 119.0,\n",
       " 850: 521.0,\n",
       " 851: 537.0,\n",
       " 852: 236.0,\n",
       " 853: 544.5,\n",
       " 854: 106.5,\n",
       " 855: 558.0,\n",
       " 856: 59.0,\n",
       " 857: 144.0,\n",
       " 858: 90.0,\n",
       " 859: 598.5,\n",
       " 860: 178.5,\n",
       " 861: 95.0,\n",
       " 862: 154.5,\n",
       " 863: 468.5,\n",
       " 864: 67.5,\n",
       " 865: 97.0,\n",
       " 866: 568.0,\n",
       " 867: 138.5,\n",
       " 868: 208.5,\n",
       " 869: 519.0,\n",
       " 870: 320.0,\n",
       " 871: 168.5,\n",
       " 872: 60.0,\n",
       " 873: 397.0,\n",
       " 874: 622.5,\n",
       " 875: 529.0,\n",
       " 876: 116.5,\n",
       " 877: 41.5,\n",
       " 878: 485.0,\n",
       " 879: 471.0,\n",
       " 880: 439.0,\n",
       " 881: 72.5,\n",
       " 882: 327.0,\n",
       " 883: 614.5,\n",
       " 884: 74.5,\n",
       " 885: 261.0,\n",
       " 886: 147.0,\n",
       " 887: 176.5,\n",
       " 888: 167.5,\n",
       " 889: 194.5,\n",
       " 890: 425.5,\n",
       " 891: 517.5,\n",
       " 892: 267.0,\n",
       " 893: 179.0,\n",
       " 894: 142.0,\n",
       " 895: 352.5,\n",
       " 896: 458.0,\n",
       " 897: 333.0,\n",
       " 898: 115.5,\n",
       " 899: 119.5,\n",
       " 900: 354.5,\n",
       " 901: 593.5,\n",
       " 902: 495.5,\n",
       " 903: 120.5,\n",
       " 904: 155.0,\n",
       " 905: 573.5,\n",
       " 906: 125.0,\n",
       " 907: 153.5,\n",
       " 908: 528.0,\n",
       " 909: 200.5,\n",
       " 910: 156.0,\n",
       " 911: 296.5,\n",
       " 912: 175.0,\n",
       " 913: 429.5,\n",
       " 914: 289.5,\n",
       " 915: 542.0,\n",
       " 916: 399.0,\n",
       " 917: 249.0,\n",
       " 918: 163.0,\n",
       " 919: 219.0,\n",
       " 920: 514.0,\n",
       " 921: 205.5,\n",
       " 922: 213.5,\n",
       " 923: 228.0,\n",
       " 924: 219.0,\n",
       " 925: 582.5,\n",
       " 926: 372.0,\n",
       " 927: 144.0,\n",
       " 928: 565.0,\n",
       " 929: 125.5,\n",
       " 930: 497.5,\n",
       " 931: 144.5,\n",
       " 932: 240.5,\n",
       " 933: 144.5,\n",
       " 934: 444.5,\n",
       " 935: 191.0,\n",
       " 936: 117.0,\n",
       " 937: 103.5,\n",
       " 938: 177.5,\n",
       " 939: 229.0,\n",
       " 940: 441.0,\n",
       " 941: 41.5}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((model.trainset.n_users))\n",
    "for u_, rank in model.ranking[u].items():\n",
    "    a[u_] = rank\n",
    "np.argsort(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
