{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pl\n",
    "\n",
    "import pyximport\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()},\n",
    "                  reload_support=True)\n",
    "from algorithms.knn_neighborhood import UserKNN\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results/protected/ml-1m_modified/privacy_risk_distribution.pkl\", \"rb\")\n",
    "privacy_risk_ml = pl.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"results/protected/ciao_modified/privacy_risk_distribution.pkl\", \"rb\")\n",
    "privacy_risk_ciao = pl.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"results/protected/douban_modified/privacy_risk_distribution.pkl\", \"rb\")\n",
    "privacy_risk_douban = pl.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"results/protected/lfm_modified/privacy_risk_distribution.pkl\", \"rb\")\n",
    "privacy_risk_lfm = pl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results/protected/douban_modified/privacy_risk_distribution.pkl\", \"rb\")\n",
    "privacy_risk_douban = pl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_douban= np.load(\"results/protected/douban_modified/thresholds.npy\")[k_idx]\n",
    "methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "\n",
    "k_idx = 1\n",
    "for m in methods:\n",
    "    for f_idx in range(5):\n",
    "        pr_douban = privacy_risk_douban[m][f_idx][k_idx]\n",
    "        frac_queries = np.sum(pr_douban[pr_douban > threshold_douban] - threshold_douban) / np.sum(pr_douban)\n",
    "        print(\"%s: %.4f\" % (m, frac_queries))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_idx = 1\n",
    "f_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-004c759f1fa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthreshold_ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/protected/ml-1m_modified/thresholds.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmethods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"userknn\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"userknn_reuse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"expect\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"expect_reuse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gain\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gain_reuse\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpr_ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprivacy_risk_ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfrac_queries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr_ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpr_ml\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold_ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mthreshold_ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr_ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "threshold_ml = np.load(\"results/protected/ml-1m_modified/thresholds.npy\")[k_idx]\n",
    "methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "for m in methods:\n",
    "    pr_ml = privacy_risk_ml[m][f_idx][k_idx]\n",
    "    frac_queries = np.sum(pr_ml[pr_ml > threshold_ml] - threshold_ml) / np.sum(pr_ml)\n",
    "    print(\"%s: %.4f\" % (m, frac_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userknn: 0.8663\n",
      "userknn_reuse: 0.8699\n",
      "expect: 0.9355\n",
      "expect_reuse: 0.9266\n",
      "gain: 0.9309\n",
      "gain_reuse: 0.9234\n"
     ]
    }
   ],
   "source": [
    "k_idx=0\n",
    "threshold_douban= np.load(\"results/protected/douban_modified/thresholds.npy\")[k_idx]\n",
    "methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "for m in methods:\n",
    "    pr_douban = privacy_risk_douban[m][f_idx][k_idx]\n",
    "    frac_queries = np.sum(pr_douban[pr_douban > threshold_douban] - threshold_douban) / np.sum(pr_douban)\n",
    "    #print(\"%s: %.4f\" % (m, frac_queries))\n",
    "    print(\"%s: %.4f\" % (m, frac_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userknn: 0.8756\n",
      "userknn_reuse: 0.8759\n",
      "expect: 0.8963\n",
      "expect_reuse: 0.8890\n",
      "gain: 0.8876\n",
      "gain_reuse: 0.8851\n"
     ]
    }
   ],
   "source": [
    "k_idx=1\n",
    "threshold_lfm = np.load(\"results/protected/lfm_modified/thresholds.npy\")[k_idx]\n",
    "methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "for m in methods:\n",
    "    pr_lfm = privacy_risk_lfm[m][f_idx][k_idx]\n",
    "    frac_queries = np.sum(pr_lfm[pr_lfm > threshold_lfm] - threshold_lfm) / np.sum(pr_lfm)\n",
    "    print(\"%s: %.4f\" % (m, frac_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_ciao = np.load(\"results/protected/ciao_modified/thresholds.npy\")[k_idx]\n",
    "methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "for m in methods:\n",
    "    pr_ciao = privacy_risk_ciao[m][f_idx][k_idx]\n",
    "    frac_queries = np.sum(pr_ciao[pr_ciao > threshold_ciao] - threshold_ciao) / np.sum(pr_ciao)\n",
    "    print(\"%s: %.4f\" % (m, frac_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = pd.read_csv(\"data/ml-1m/ratings.dat\", sep=\"::\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "douban_df = pd.read_csv(\"data/douban/douban.csv\", sep=\";\", names=[\"user_id\", \"item_id\", \"rating\"])\n",
    "lfm_df = pd.read_csv(\"data/lfm/artist_ratings.csv\", sep=\";\", names=[\"user_id\", \"item_id\", \"rating\"])\n",
    "ciao_df = pd.read_csv(\"data/ciao/ciao.csv\", sep=\";\", names=[\"user_id\", \"item_id\", \"rating\"])\n",
    "goodreads_df = pd.read_csv(\"data/goodreads/sample.csv\", sep=\";\", names=[\"user_id\", \"item_id\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_profile_size_ml = ml_df.groupby(\"user_id\").size() / len(ml_df)\n",
    "rel_profile_size_douban = douban_df.groupby(\"user_id\").size() / len(douban_df)\n",
    "rel_profile_size_lfm = lfm_df.groupby(\"user_id\").size() / len(lfm_df)\n",
    "rel_profile_size_ciao = ciao_df.groupby(\"user_id\").size() / len(ciao_df)\n",
    "rel_profile_size_goodreads = goodreads_df.groupby(\"user_id\").size() / len(goodreads_df)\n",
    "\n",
    "n = 50\n",
    "plt.plot(sorted(rel_profile_size_ml, reverse=True)[:n], label=\"MovieLens 1M\")\n",
    "plt.scatter(range(n), sorted(rel_profile_size_ml, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_profile_size_douban, reverse=True)[:n], label=\"Douban\")\n",
    "plt.scatter(range(n), sorted(rel_profile_size_douban, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_profile_size_lfm, reverse=True)[:n], label=\"LastFM\")\n",
    "plt.scatter(range(n), sorted(rel_profile_size_lfm, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_profile_size_ciao, reverse=True)[:n], label=\"Ciao\")\n",
    "plt.scatter(range(n), sorted(rel_profile_size_ciao, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_profile_size_goodreads, reverse=True)[:n], label=\"Goodreads\")\n",
    "plt.scatter(range(n), sorted(rel_profile_size_goodreads, reverse=True)[:n], s=20)\n",
    "plt.ylabel(\"Frac. of Ratings\")\n",
    "plt.xlabel(\"User\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_item_profile_size_ml = ml_df.groupby(\"item_id\").size() / ml_df[\"user_id\"].nunique()\n",
    "rel_item_profile_size_douban = douban_df.groupby(\"item_id\").size() / douban_df[\"user_id\"].nunique()\n",
    "rel_item_profile_size_lfm = lfm_df.groupby(\"item_id\").size() / lfm_df[\"user_id\"].nunique()\n",
    "rel_item_profile_size_ciao = ciao_df.groupby(\"item_id\").size() / ciao_df[\"user_id\"].nunique()\n",
    "rel_item_profile_size_goodreads = goodreads_df.groupby(\"item_id\").size() / goodreads_df[\"user_id\"].nunique()\n",
    "\n",
    "n = 50\n",
    "plt.plot(sorted(rel_item_profile_size_ml, reverse=True)[:n], label=\"MovieLens 1M\")\n",
    "plt.scatter(range(n), sorted(rel_item_profile_size_ml, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_item_profile_size_douban, reverse=True)[:n], label=\"Douban\")\n",
    "plt.scatter(range(n), sorted(rel_item_profile_size_douban, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_item_profile_size_lfm, reverse=True)[:n], label=\"LastFM\")\n",
    "plt.scatter(range(n), sorted(rel_item_profile_size_lfm, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_item_profile_size_ciao, reverse=True)[:n], label=\"Ciao\")\n",
    "plt.scatter(range(n), sorted(rel_item_profile_size_ciao, reverse=True)[:n], s=20)\n",
    "plt.plot(sorted(rel_item_profile_size_goodreads, reverse=True)[:n], label=\"Goodreads\")\n",
    "plt.scatter(range(n), sorted(rel_item_profile_size_goodreads, reverse=True)[:n], s=20)\n",
    "plt.ylabel(\"Item Popularity\")\n",
    "plt.xlabel(\"Items\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    methods = [\"userknn\", \"userknn_reuse\", \"expect\", \"expect_reuse\", \"gain\", \"gain_reuse\"]\n",
    "    Ks = np.load(\"results/unprotected/\" + dataset_name + \"/K.npy\")\n",
    "    Ks_dp = np.load(\"results/protected/\" + dataset_name + \"/K.npy\")\n",
    "    \n",
    "    rel_mae_loss = dict()\n",
    "    abs_mae_loss = dict()\n",
    "    for m in methods:\n",
    "        mae = np.load(\"results/unprotected/\" + dataset_name + \"/mae_\" + m + \".npy\")\n",
    "        mae_dp = np.load(\"results/protected/\" + dataset_name + \"/mae_\" + m + \".npy\")\n",
    "        rel_mae_loss[m] = mae_dp / mae\n",
    "        abs_mae_loss[m] = mae_dp - mae\n",
    "\n",
    "    return {\"K\": Ks,\n",
    "            \"K_dp\": Ks_dp,\n",
    "            \"rel_mae_loss\": rel_mae_loss,\n",
    "            \"abs_mae_loss\": abs_mae_loss}\n",
    "\n",
    "ml_data = load_data(\"ml-1m\")\n",
    "douban_data = load_data(\"douban\")\n",
    "lfm_data = load_data(\"lfm\")\n",
    "ciao_data = load_data(\"ciao\")\n",
    "goodreads_data = load_data(\"goodreads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_loss(data, method, k_idx=1):\n",
    "    return data[\"rel_mae_loss\"][method][k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width of the bars\n",
    "barWidth = 0.1\n",
    "\n",
    "# The x position of bars\n",
    "n_datasets = 4\n",
    "r1 = np.arange(n_datasets)\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    "\n",
    "rel_loss_userknn = np.array([rel_loss(ml_data, \"userknn\"), rel_loss(douban_data, \"userknn\"), rel_loss(ciao_data, \"userknn\"), rel_loss(goodreads_data, \"userknn\")])\n",
    "rel_loss_userknn_reuse = np.array([rel_loss(ml_data, \"userknn_reuse\"), rel_loss(douban_data, \"userknn_reuse\"), rel_loss(ciao_data, \"userknn_reuse\"), rel_loss(goodreads_data, \"userknn_reuse\")])\n",
    "rel_loss_expect = np.array([rel_loss(ml_data, \"expect\"), rel_loss(douban_data, \"expect\"), rel_loss(ciao_data, \"expect\"), rel_loss(goodreads_data, \"expect\")])\n",
    "rel_loss_expect_reuse = np.array([rel_loss(ml_data, \"expect_reuse\"), rel_loss(douban_data, \"expect_reuse\"), rel_loss(ciao_data, \"expect_reuse\"), rel_loss(goodreads_data, \"expect_reuse\")])\n",
    "rel_loss_gain = np.array([rel_loss(ml_data, \"gain\"), rel_loss(douban_data, \"gain\"), rel_loss(ciao_data, \"gain\"), rel_loss(goodreads_data, \"gain\")])\n",
    "rel_loss_gain_reuse = np.array([rel_loss(ml_data, \"gain_reuse\"), rel_loss(douban_data, \"gain_reuse\"), rel_loss(ciao_data, \"gain_reuse\"), rel_loss(goodreads_data, \"gain_reuse\")])\n",
    "\n",
    "plt.bar(r1, rel_loss_userknn - 1, width=barWidth, color = 'C0', edgecolor = 'black', capsize=7, label='UserKNN', alpha=0.5)\n",
    "plt.bar(r2, rel_loss_userknn_reuse - 1, width=barWidth, color = 'C0', edgecolor = 'black', capsize=7, label='UserKNN+Reuse')\n",
    "plt.bar(r3, rel_loss_expect - 1, width=barWidth, color = 'C1', edgecolor = 'black', capsize=7, label='Expect', alpha=0.5)\n",
    "plt.bar(r4, rel_loss_expect_reuse - 1, width=barWidth, color = 'C1', edgecolor = 'black', capsize=7, label='Expect+Reuse')\n",
    "plt.bar(r5, rel_loss_gain_reuse - 1, width=barWidth, color = 'C2', edgecolor = 'black', capsize=7, label='Gain', alpha=0.5)\n",
    "plt.bar(r6, rel_loss_gain_reuse - 1, width=barWidth, color = 'C2', edgecolor = 'black', capsize=7, label='Gain+Reuse')\n",
    " \n",
    "# general layout\n",
    "plt.xticks([r + barWidth for r in range(n_datasets)], [\"MovieLens 1M\", \"Douban\", \"Ciao\", \"Goodreads\"])\n",
    "plt.legend(ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml, db, lfm, ciao, gr = [], [], [], [], []\n",
    "for p in np.arange(0, 100, 1):\n",
    "    ml.append(np.percentile(rel_profile_size_ml, p))\n",
    "    db.append(np.percentile(rel_profile_size_douban, p))\n",
    "    lfm.append(np.percentile(rel_profile_size_lfm, p))\n",
    "    ciao.append(np.percentile(rel_profile_size_ciao, p))\n",
    "    gr.append(np.percentile(rel_profile_size_goodreads, p))\n",
    "\n",
    "plt.plot(ml, label=\"MovieLens 1M\")\n",
    "plt.plot(db, label=\"Douban\")\n",
    "plt.plot(lfm, label=\"LastFM\")\n",
    "plt.plot(ciao, label=\"Ciao\")\n",
    "plt.plot(gr, label=\"Goodreads\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Percentage of Users\")\n",
    "plt.ylabel(\"Rel. Profile Size\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.round(len(rel_profile_size_ml) * 0.05).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_ml)[-n:]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_douban) * 0.05).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_douban)[-n:]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_lfm) * 0.05).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_lfm)[-n:]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_ciao) * 0.05).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_ciao)[-n:]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_goodreads) * 0.05).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_goodreads)[-n:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.round(len(rel_profile_size_ml) * 0.95).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_ml)[:n]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_douban) * 0.95).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_douban)[:n]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_lfm) * 0.95).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_lfm)[:n]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_ciao) * 0.95).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_ciao)[:n]))\n",
    "\n",
    "n = np.round(len(rel_profile_size_goodreads) * 0.95).astype(int)\n",
    "print(np.mean(sorted(rel_profile_size_goodreads)[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
