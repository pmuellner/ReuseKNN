{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "import numpy as np\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()},\n",
    "                  reload_support=True)\n",
    "from algorithms.knn_neighborhood import UserKNN\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime as dt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/ml-100k/u.data\", sep=\"\\t\")\n",
    "data_df.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data_df.drop(columns=[\"timestamp\"], axis=1, inplace=True)\n",
    "data_df[\"user_id\"] = data_df[\"user_id\"].map({b: a for a, b in enumerate(data_df[\"user_id\"].unique())})\n",
    "data_df[\"item_id\"] = data_df[\"item_id\"].map({b: a for a, b in enumerate(data_df[\"item_id\"].unique())})\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "dataset = Dataset.load_from_df(data_df, reader=reader)\n",
    "trainset, testset = train_test_split(dataset, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=5] UserKNN 0.600000\n",
      "[k=5] UserKNN+Reuse 0.600000\n",
      "[k=5] Popularity 0.900000\n",
      "[k=5] Popularity+Reuse 0.900000\n",
      "[k=5] Gain 0.800000\n",
      "[k=5] Gain+Reuse 0.800000\n",
      "[k=10] UserKNN 0.650000\n",
      "[k=10] UserKNN+Reuse 0.700000\n",
      "[k=10] Popularity 0.800000\n",
      "[k=10] Popularity+Reuse 0.800000\n",
      "[k=10] Gain 0.750000\n",
      "[k=10] Gain+Reuse 0.800000\n",
      "[k=15] UserKNN 0.700000\n",
      "[k=15] UserKNN+Reuse 0.600000\n",
      "[k=15] Popularity 0.850000\n",
      "[k=15] Popularity+Reuse 0.750000\n",
      "[k=15] Gain 0.700000\n",
      "[k=15] Gain+Reuse 0.650000\n",
      "[k=20] UserKNN 0.700000\n",
      "[k=20] UserKNN+Reuse 0.600000\n",
      "[k=20] Popularity 0.800000\n",
      "[k=20] Popularity+Reuse 0.750000\n",
      "[k=20] Gain 0.700000\n",
      "[k=20] Gain+Reuse 0.650000\n"
     ]
    }
   ],
   "source": [
    "sim = UserKNN.compute_similarities(trainset, min_support=1)\n",
    "pop = UserKNN.compute_popularities(trainset)\n",
    "gain = UserKNN.compute_gain(trainset)\n",
    "\n",
    "userknn_frac_predicted, userknnreuse_frac_predicted = [], []\n",
    "pop_frac_predicted, popreuse_frac_predicted = [], []\n",
    "gain_frac_predicted, gainreuse_frac_predicted = [], []\n",
    "\n",
    "Ks = [5, 10, 15, 20]#np.arange(1, 30, 2) \n",
    "for k in Ks:\n",
    "    model = UserKNN(k=k, precomputed_sim=sim)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] UserKNN %f\" % (k, frac))\n",
    "    userknn_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] UserKNN+Reuse %f\" % (k, frac))\n",
    "    userknnreuse_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] Popularity %f\" % (k, frac))\n",
    "    pop_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, reuse=True, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] Popularity+Reuse %f\" % (k, frac))\n",
    "    popreuse_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] Gain %f\" % (k, frac))\n",
    "    gain_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, reuse=True, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] Gain+Reuse %f\" % (k, frac))\n",
    "    gainreuse_frac_predicted.append(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frac. of predicted high-exposed neighbors')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.plot(Ks, userknn_frac_predicted, color=\"C0\", linestyle=\"dashed\", label=\"UserKNN\", alpha=0.5)\n",
    "plt.plot(Ks, userknnreuse_frac_predicted, color=\"C1\", linestyle=\"dashed\", label=\"Popularity\", alpha=0.5)\n",
    "plt.plot(Ks, pop_frac_predicted, color=\"C2\", linestyle=\"dashed\", label=\"Gain\", alpha=0.5)\n",
    "plt.plot(Ks, popreuse_frac_predicted, color=\"C0\", linestyle=\"solid\", label=\"UserKNN + Reuse\")\n",
    "plt.plot(Ks, gain_frac_predicted, color=\"C1\", linestyle=\"solid\", label=\"Popularity + Reuse\")\n",
    "plt.plot(Ks, gainreuse_frac_predicted, color=\"C2\", linestyle=\"solid\", label=\"Gain + Reuse\")\n",
    "plt.legend(ncol=2)\n",
    "plt.xlabel(\"Nr. of Neighbors\")\n",
    "plt.ylabel(\"Frac. of predicted high-exposed neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=5] UserKNN 0.950000\n",
      "[k=10] UserKNN 0.950000\n",
      "[k=15] UserKNN 0.950000\n",
      "[k=20] UserKNN 0.900000\n"
     ]
    }
   ],
   "source": [
    "sim = UserKNN.compute_similarities(trainset, min_support=1)\n",
    "Ks = [5, 10, 15, 20]\n",
    "for k in Ks:\n",
    "    model = UserKNN(k=k, precomputed_sim=sim, tau_2=0.5, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in enumerate(model.n_queries):\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            for bob, _ in k_neighbors:\n",
    "                exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:20]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:20]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 20\n",
    "    print(\"[k=%d] UserKNN %f\" % (k, frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
