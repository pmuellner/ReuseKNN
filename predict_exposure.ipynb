{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "import numpy as np\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()},\n",
    "                  reload_support=True)\n",
    "from algorithms.knn_neighborhood import UserKNN\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime as dt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/ml-100k/u.data\", sep=\"\\t\")\n",
    "data_df.columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data_df.drop(columns=[\"timestamp\"], axis=1, inplace=True)\n",
    "data_df[\"user_id\"] = data_df[\"user_id\"].map({b: a for a, b in enumerate(data_df[\"user_id\"].unique())})\n",
    "data_df[\"item_id\"] = data_df[\"item_id\"].map({b: a for a, b in enumerate(data_df[\"item_id\"].unique())})\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "dataset = Dataset.load_from_df(data_df, reader=reader)\n",
    "trainset, testset = train_test_split(dataset, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=1] UserKNN 0.600000\n",
      "[k=1] UserKNN+Reuse 0.500000\n",
      "[k=1] Popularity 0.700000\n",
      "[k=1] Popularity+Reuse 0.700000\n",
      "[k=1] Gain 0.900000\n",
      "[k=1] Gain+Reuse 0.900000\n",
      "[k=3] UserKNN 0.700000\n",
      "[k=3] UserKNN+Reuse 0.100000\n",
      "[k=3] Popularity 0.900000\n",
      "[k=3] Popularity+Reuse 0.900000\n",
      "[k=3] Gain 0.900000\n",
      "[k=3] Gain+Reuse 0.900000\n",
      "[k=5] UserKNN 0.500000\n",
      "[k=5] UserKNN+Reuse 0.200000\n",
      "[k=5] Popularity 0.900000\n",
      "[k=5] Popularity+Reuse 0.900000\n",
      "[k=5] Gain 1.000000\n",
      "[k=5] Gain+Reuse 1.000000\n",
      "[k=7] UserKNN 0.500000\n",
      "[k=7] UserKNN+Reuse 0.300000\n",
      "[k=7] Popularity 0.900000\n",
      "[k=7] Popularity+Reuse 0.900000\n",
      "[k=7] Gain 0.900000\n",
      "[k=7] Gain+Reuse 0.900000\n",
      "[k=9] UserKNN 0.600000\n",
      "[k=9] UserKNN+Reuse 0.600000\n",
      "[k=9] Popularity 0.900000\n",
      "[k=9] Popularity+Reuse 0.900000\n",
      "[k=9] Gain 0.700000\n",
      "[k=9] Gain+Reuse 0.700000\n",
      "[k=11] UserKNN 0.800000\n",
      "[k=11] UserKNN+Reuse 0.800000\n",
      "[k=11] Popularity 0.900000\n",
      "[k=11] Popularity+Reuse 0.900000\n",
      "[k=11] Gain 0.700000\n",
      "[k=11] Gain+Reuse 0.700000\n",
      "[k=13] UserKNN 0.800000\n",
      "[k=13] UserKNN+Reuse 0.700000\n",
      "[k=13] Popularity 0.900000\n",
      "[k=13] Popularity+Reuse 0.900000\n",
      "[k=13] Gain 0.700000\n",
      "[k=13] Gain+Reuse 0.700000\n",
      "[k=15] UserKNN 0.700000\n",
      "[k=15] UserKNN+Reuse 0.600000\n",
      "[k=15] Popularity 0.900000\n",
      "[k=15] Popularity+Reuse 0.900000\n",
      "[k=15] Gain 0.700000\n"
     ]
    }
   ],
   "source": [
    "sim = UserKNN.compute_similarities(trainset, min_support=1)\n",
    "pop = UserKNN.compute_popularities(trainset)\n",
    "gain = UserKNN.compute_gain(trainset)\n",
    "\n",
    "userknn_frac_predicted, userknnreuse_frac_predicted = [], []\n",
    "pop_frac_predicted, popreuse_frac_predicted = [], []\n",
    "gain_frac_predicted, gainreuse_frac_predicted = [], []\n",
    "\n",
    "Ks = np.arange(1, 30, 2)\n",
    "for k in Ks:\n",
    "    model = UserKNN(k=k, precomputed_sim=sim)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] UserKNN %f\" % (k, frac))\n",
    "    userknn_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, reuse=True)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] UserKNN+Reuse %f\" % (k, frac))\n",
    "    userknnreuse_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] Popularity %f\" % (k, frac))\n",
    "    pop_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_pop=pop, tau_2=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] Popularity+Reuse %f\" % (k, frac))\n",
    "    popreuse_frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] Gain %f\" % (k, frac))\n",
    "    frac_predicted.append(frac)\n",
    "    \n",
    "    model = UserKNN(k=k, precomputed_sim=sim, precomputed_gain=gain, tau_4=0.5)\n",
    "    model.fit(trainset)\n",
    "    _ = model.test(testset)\n",
    "\n",
    "    exposure_real = np.zeros((trainset.n_users))\n",
    "    for uid, exposure in model.exposure_u.items():\n",
    "        exposure_real[uid] = exposure\n",
    "\n",
    "    exposure_est = np.zeros((trainset.n_users))\n",
    "    for alice, ratings in trainset.ur.items():\n",
    "        ranks =  model.ranking[alice]\n",
    "        N_alice = set()\n",
    "        for iid, _ in ratings:\n",
    "            possible_neighbors = [(bob, ranks[bob]) for bob, _ in trainset.ir[iid] if bob != alice]\n",
    "            k_neighbors = heapq.nlargest(k, possible_neighbors, key=lambda t: t[1])\n",
    "            N_alice = N_alice.union([bob for bob, _ in k_neighbors])\n",
    "        for bob in N_alice:\n",
    "            exposure_est[bob] += 1\n",
    "\n",
    "    top_vulnerable_real = np.argsort(exposure_real)[::-1][:10]\n",
    "    top_vulnerable_est = np.argsort(exposure_est)[::-1][:10]\n",
    "\n",
    "    frac = len(set(top_vulnerable_real).intersection(top_vulnerable_est)) / 10\n",
    "    print(\"[k=%d] Gain+Reuse %f\" % (k, frac))\n",
    "    gainreuse_frac_predicted.append(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ks, userknn_frac_predicted, color=\"C0\", linestyle=\"dashed\", label=\"UserKNN\", alpha=0.5)\n",
    "plt.plot(Ks, userknnreuse_frac_predicted, color=\"C1\", linestyle=\"dashed\", label=\"Popularity\", alpha=0.5)\n",
    "plt.plot(Ks, pop_frac_predicted, color=\"C2\", linestyle=\"dashed\", label=\"Gain\", alpha=0.5)\n",
    "plt.plot(Ks, popreuse_frac_predicted, color=\"C0\", linestyle=\"solid\", label=\"UserKNN + Reuse\")\n",
    "plt.plot(Ks, gain_frac_predicted, color=\"C1\", linestyle=\"solid\", label=\"Popularity + Reuse\")\n",
    "plt.plot(Ks, gainreuse_frac_predicted, color=\"C2\", linestyle=\"solid\", label=\"Gain + Reuse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
